"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞ —Å eval_lib
–ú–µ—Ç—Ä–∏–∫–∏: AnswerRelevancy, ToolCorrectness, TaskSuccessRate
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio  # noqa: E402
import time  # noqa: E402
from typing import List, Optional  # noqa: E402
from eval_lib import (
    evaluate,
    EvalTestCase,
    AnswerRelevancyMetric,
    ToolCorrectnessMetric,
    TaskSuccessRateMetric,
    CustomLLMClient
)  # noqa: E402
from dataset_parser import DatasetParser  # noqa: E402
from agent_connector import AgentConnector  # noqa: E402
from custom_proxy_llm import create_proxy_llm  # noqa: E402


async def evaluate_agent_with_eval_lib(
    excel_path: str,
    agent_connector: AgentConnector,
    metrics_list: List[str],
    urls: Optional[List[str]] = None,
    model: str | CustomLLMClient = "gpt-4o-mini",
    threshold: float = 0.7,
    temperature: float = 0.5,
    sleep_time: float = 0.5,
    verbose: bool = True,
    show_dashboard: bool = False,
    session_name: str = "Agent Evaluation"
):
    """
    –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º eval_lib

    Args:
        excel_path: –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        agent_connector: –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä –∫ –∞–≥–µ–Ω—Ç—É
        metrics_list: —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        urls: —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∞–≥–µ–Ω—Ç—É
        model: –º–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫
        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è LLM
        sleep_time: –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        verbose: –¥–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
        show_dashboard: –ø–æ–∫–∞–∑–∞—Ç—å dashboard
        session_name: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
    """

    print("\n" + "=" * 70)
    print(f"üöÄ {session_name.upper()}")
    print("=" * 70)

    # ============= –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ =============
    print("\nüìÇ –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    parser = DatasetParser()
    df = parser.load_dataset(excel_path)

    if df is None:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return None

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    info = parser.validate_dataset(df)
    print(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {info['total_rows']}")
    print(f"   ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä: {info['valid_pairs']}")
    if info['has_expected_tools_column']:
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—É–ª–æ–≤: {info['avg_tools_count']:.1f}")
        print(f"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—É–ª–æ–≤: {len(info['unique_tools'])}")

    # –ü—Ä–µ–≤—å—é
    parser.preview_dataset(df, n=2)

    # ============= –®–ê–ì 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö =============
    print("\nüìù –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    pairs = parser.get_question_response_pairs(df)
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(pairs)} –ø–∞—Ä")

    if urls:
        print(f"üîó URLs –¥–ª—è –∞–≥–µ–Ω—Ç–∞: {urls}")

    # ============= –®–ê–ì 3: –ó–∞–ø—Ä–æ—Å—ã –∫ –∞–≥–µ–Ω—Ç—É =============
    print("\nü§ñ –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–∞...")

    test_cases = []

    for i, pair in enumerate(pairs, 1):
        question = pair['question']
        expected_response = pair['expected_response']
        expected_tools = pair['expected_tools']

        print(f"\n[{i}/{len(pairs)}] {question[:60]}...")

        # –ó–∞–ø—Ä–æ—Å –∫ –∞–≥–µ–Ω—Ç—É —Å URLs
        response = agent_connector.query(question, urls=urls)

        if response.get('error'):
            print(f"   ‚ùå {response['error']}")
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        answer = response.get('output', '')
        tools_used = response.get('tools_used', [])

        print(f"   ‚úÖ –û—Ç–≤–µ—Ç: {answer[:50]}...")
        print(f"   üîß –¢—É–ª—ã: {tools_used}")

        # –°–æ–∑–¥–∞–µ–º EvalTestCase
        test_case = EvalTestCase(
            input=question,
            actual_output=answer,
            expected_output=expected_response,
            tools_called=tools_used,  # —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
            expected_tools=expected_tools  # —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
        )

        test_cases.append(test_case)

        if i < len(pairs):
            time.sleep(sleep_time)

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç –∫–µ–π—Å–æ–≤")

    # ============= –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ =============
    print("\nüìä –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {metrics_list}")
    print(f"   –ü–æ—Ä–æ–≥: {threshold}")
    print(f"   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
    model_name = model.get_model_name() if isinstance(
        model, CustomLLMClient) else model
    print(f"   –ú–æ–¥–µ–ª—å: {model_name}")

    metrics = []
    metric_classes = {
        'answer_relevancy': AnswerRelevancyMetric,
        'tool_correctness': ToolCorrectnessMetric,
        'task_success_rate': TaskSuccessRateMetric
    }

    for metric_name in metrics_list:
        if metric_name == 'tool_correctness':
            metric = ToolCorrectnessMetric(
                threshold=threshold,
                verbose=verbose,
                exact_match=True,
                check_ordering=True
            )
            metrics.append(metric)
            print(f"   ‚úÖ ToolCorrectnessMetric")
        elif metric_name in metric_classes:
            metric = metric_classes[metric_name](
                model=model,
                threshold=threshold,
                temperature=temperature,
                verbose=verbose
            )
            metrics.append(metric)
            print(f"   ‚úÖ {metric_name}")

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(metrics)} –º–µ—Ç—Ä–∏–∫")

    # ============= –®–ê–ì 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ =============
    print("\nüß™ –®–∞–≥ 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏...")
    print(f"   –¢–µ—Å—Ç –∫–µ–π—Å–æ–≤: {len(test_cases)}")
    print(f"   –ú–µ—Ç—Ä–∏–∫: {len(metrics)}")
    print("=" * 70)

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
        results = await evaluate(
            test_cases=test_cases,
            metrics=metrics,
            verbose=verbose,
            show_dashboard=show_dashboard,
            session_name=session_name
        )

        print("\n" + "=" * 70)
        print("‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 70)

        return results

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return None


async def scenario_1():
    """–°—Ü–µ–Ω–∞—Ä–∏–π 2: Proxy –º–æ–¥–µ–ª—å"""

    print("\n" + "=" * 70)
    print("üìã –°–¶–ï–ù–ê–†–ò–ô 2: –û—Ü–µ–Ω–∫–∞ —Å Proxy –º–æ–¥–µ–ª—å—é")
    print("=" * 70)

    # –ê–≥–µ–Ω—Ç
    agent = AgentConnector(
        endpoint_url="http://5.11.83.110:8004/ask",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        user_id="AleksM",
        session_id="faecb783-d996-49ee-97a0-f13805f63a52"
    )

    # Proxy –º–æ–¥–µ–ª—å
    proxy_model = create_proxy_llm(
        model="gpt-4o-mini",
        api_key="sk-proxy-maEZp5Yp0-h9nOHDZXtoZPql5VRW3CqTqakKOQgsQtQ",
        base_url="http://5.11.83.110:8000"
    )

    # –î–∞—Ç–∞—Å–µ—Ç
    excel_path = "data/evaluation_dataset.xlsx"

    # URLs –¥–ª—è –∞–≥–µ–Ω—Ç–∞
    test_urls = [
        "https://www.rentalads.com/apartments-for-rent/ny/new-york/"
    ]

    # –ú–µ—Ç—Ä–∏–∫–∏
    metrics_to_use = [
        'answer_relevancy',
        'tool_correctness',
        'task_success_rate'
    ]

    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
    results = await evaluate_agent_with_eval_lib(
        excel_path=excel_path,
        agent_connector=agent,
        metrics_list=metrics_to_use,
        urls=test_urls,
        model=proxy_model,  # –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "gpt-4o-mini"
        threshold=0.7,
        temperature=0.5,
        sleep_time=0.5,
        verbose=True,
        show_dashboard=True,
        session_name="Agent Evaluation - Proxy"
    )

    return results


if __name__ == "__main__":

    asyncio.run(scenario_1())
