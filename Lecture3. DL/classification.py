import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, top_k_accuracy_score
from scipy.stats import entropy as scipy_entropy
from torchmetrics.classification import MulticlassCalibrationError, MulticlassAccuracy
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (CIFAR-10)
# ============================================================================


def load_cifar10_data(batch_size=128, val_split=0.1):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ CIFAR-10 —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ train/validation/test

    CIFAR-10 —Å–æ–¥–µ—Ä–∂–∏—Ç 60,000 —Ü–≤–µ—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 32x32 –ø–∏–∫—Å–µ–ª—è –≤ 10 –∫–ª–∞—Å—Å–∞—Ö:
    - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck

    Args:
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        val_split: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ (0.1 = 10%)
    """

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
    transform_train = transforms.Compose([
        # RandomHorizontalFlip: —Å–ª—É—á–∞–π–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∞–º–æ–ª—ë—Ç –≤–ª–µ–≤–æ/–≤–ø—Ä–∞–≤–æ).
        transforms.RandomHorizontalFlip(p=0.5),
        # RandomCrop: —Å–ª—É—á–∞–π–Ω–æ –≤—ã—Ä–µ–∑–∞–µ—Ç —á–∞—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–º–∫—É (—á—Ç–æ–±—ã –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ä–∞–∫—É—Ä—Å—ã).
        transforms.RandomCrop(32, padding=4),
        # ToTensor: –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ PIL/NumPy –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch.
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    ])

    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
    transform_val_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2023, 0.1994, 0.2010))
    ])

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    full_train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_val_test
    )

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞ train –∏ validation
    train_size = int((1 - val_split) * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size

    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    generator = torch.Generator().manual_seed(42)
    train_dataset, val_dataset_temp = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size], generator=generator
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=False, transform=transform_val_test
    )

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    val_indices = val_dataset_temp.indices
    val_dataset = torch.utils.data.Subset(val_dataset, val_indices)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ CIFAR-10
    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

    print(f"–†–∞–∑–º–µ—Ä—ã –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  - –û–±—É—á–∞—é—â–∏–π: {len(train_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π: {len(val_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  - –¢–µ—Å—Ç–æ–≤—ã–π: {len(test_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")

    return train_loader, val_loader, test_loader, classes


# def load_mnist_data(batch_size=128, val_split=0.1):
#     """
#     –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ train/validation/test

#     MNIST —Å–æ–¥–µ—Ä–∂–∏—Ç 70,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä 28x28 –ø–∏–∫—Å–µ–ª–µ–π –≤ –æ—Ç—Ç–µ–Ω–∫–∞—Ö —Å–µ—Ä–æ–≥–æ:
#     - —Ü–∏—Ñ—Ä—ã –æ—Ç 0 –¥–æ 9

#     Args:
#         batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
#         val_split: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ (0.1 = 10%)
#     """

#     # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
#     transform_train = transforms.Compose([
#         # –°–ª—É—á–∞–π–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏ (–¥–ª—è —Ü–∏—Ñ—Ä –º–µ–Ω–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ)
#         transforms.RandomHorizontalFlip(p=0.1),  # –£–º–µ–Ω—å—à–∏–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
#         # –°–ª—É—á–∞–π–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º
#         transforms.RandomCrop(28, padding=2),     # –ú–µ–Ω—å—à–∏–π padding –¥–ª—è 28x28
#         # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–≤–æ—Ä–æ—Ç (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ü–∏—Ñ—Ä)
#         transforms.RandomRotation(degrees=10),
#         transforms.ToTensor(),                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä
#         #  –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è MNIST
#         transforms.Normalize((0.1307,), (0.3081,))
#     ])

#     # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
#     transform_val_test = transforms.Compose([
#         transforms.ToTensor(),
#         #  –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è MNIST
#         transforms.Normalize((0.1307,), (0.3081,))
#     ])

#     # –ó–∞–≥—Ä—É–∑–∫–∞ MNIST –≤–º–µ—Å—Ç–æ CIFAR-10
#     full_train_dataset = torchvision.datasets.MNIST(
#         root='./data', train=True, download=True, transform=transform_train
#     )
#     test_dataset = torchvision.datasets.MNIST(
#         root='./data', train=False, download=True, transform=transform_val_test
#     )

#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞ train –∏ validation (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
#     train_size = int((1 - val_split) * len(full_train_dataset))
#     val_size = len(full_train_dataset) - train_size

#     # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
#     generator = torch.Generator().manual_seed(42)
#     train_dataset, val_dataset_temp = torch.utils.data.random_split(
#         full_train_dataset, [train_size, val_size], generator=generator
#     )

#     # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
#     val_dataset = torchvision.datasets.MNIST(
#         root='./data', train=True, download=False, transform=transform_val_test
#     )

#     # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
#     val_indices = val_dataset_temp.indices
#     val_dataset = torch.utils.data.Subset(val_dataset, val_indices)

#     # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
#     train_loader = DataLoader(
#         train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
#     val_loader = DataLoader(
#         val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
#     test_loader = DataLoader(
#         test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

#     # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ MNIST
#     classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')

#     print(f"–†–∞–∑–º–µ—Ä—ã –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
#     print(f"  - –û–±—É—á–∞—é—â–∏–π: {len(train_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
#     print(f"  - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π: {len(val_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
#     print(f"  - –¢–µ—Å—Ç–æ–≤—ã–π: {len(test_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")

#     return train_loader, val_loader, test_loader, classes

# ============================================================================
# 2. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò
# ============================================================================


class SimpleCNN_CIFAR(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ CIFAR
    """

    def __init__(self, num_classes=10):
        super(SimpleCNN_CIFAR, self).__init__()

        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç 3 –∫–∞–Ω–∞–ª–∞ –≤–º–µ—Å—Ç–æ 1
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)     # 32x32x3 -> 32x32x32
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # 32x32x32 -> 32x32x64
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # 16x16x64 -> 16x16x128

        # –ü—É–ª–∏–Ω–≥ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

        #  –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ —Ä–∞–∑–º–µ—Ä 8x8x128
        # 8x8x128 -> 256 (–º–µ–Ω—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤)
        self.fc1 = nn.Linear(128 * 8 * 8, 256)
        self.fc2 = nn.Linear(256, 64)           # 256 -> 64
        self.fc3 = nn.Linear(64, num_classes)   # 64 -> 10 (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤)

    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –∏ –ø—É–ª–∏–Ω–≥–æ–º
        x = self.pool(self.relu(self.conv1(x)))  # 28x28x32 -> 14x14x32
        x = self.pool(self.relu(self.conv2(x)))  # 14x14x64 -> 7x7x64
        x = self.relu(self.conv3(x))             # 7x7x128 (–ë–ï–ó –ø—É–ª–∏–Ω–≥–∞)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        x = x.view(x.size(0), -1)  # Flatten: batch_size x (7*7*128)

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ª–æ–≥–∏—Ç—ã)

        return x


# class VerySimpleCNN(nn.Module):
#     """
#     –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
#     """

#     def __init__(self, num_classes=10):
#         super(VerySimpleCNN, self).__init__()

#         self.conv1 = nn.Conv2d(1, 16, 5)  # 28x28 -> 24x24
#         self.pool = nn.MaxPool2d(2, 2)    # 24x24 -> 12x12
#         self.conv2 = nn.Conv2d(16, 32, 5)  # 12x12 -> 8x8
#         self.fc1 = nn.Linear(32 * 4 * 4, 64)
#         self.fc2 = nn.Linear(64, num_classes)

#     def forward(self, x):
#         x = self.pool(torch.relu(self.conv1(x)))
#         x = self.pool(torch.relu(self.conv2(x)))
#         x = x.view(-1, 32 * 4 * 4)
#         x = torch.relu(self.fc1(x))
#         x = self.fc2(x)
#         return x


# class SimpleCNN(nn.Module):
#     """
#     –ü—Ä–æ—Å—Ç–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π CIFAR-10
#     """

#     def __init__(self, num_classes=10):
#         super(SimpleCNN, self).__init__()
#         # 1) –≤—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ = 1
#         self.conv1 = nn.Conv2d(1, 32, 3, padding=1)     # 28x28 -> 28x28
#         self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # 14x14 -> 14x14
#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # 7x7   -> 7x7
#         # 3x3   -> 3x3 (–ø–æ—Å–ª–µ 3 –ø—É–ª–æ–≤)
#         self.conv4 = nn.Conv2d(128, 256, 3, padding=1)

#         self.pool = nn.MaxPool2d(2, 2)
#         self.relu = nn.ReLU()
#         self.dropout = nn.Dropout(0.5)

#         # 2) —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Ç—Ä–µ—Ö –ø—É–ª–æ–≤ –Ω–∞ 28x28 = 3x3
#         self.fc1 = nn.Linear(256 * 3 * 3, 512)
#         self.fc2 = nn.Linear(512, 128)
#         self.fc3 = nn.Linear(128, num_classes)

#     def forward(self, x):
#         # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –∏ –ø—É–ª–∏–Ω–≥–æ–º
#         x = self.pool(self.relu(self.conv1(x)))  # 32x32x32 -> 16x16x32
#         x = self.pool(self.relu(self.conv2(x)))  # 16x16x64 -> 8x8x64
#         x = self.pool(self.relu(self.conv3(x)))  # 8x8x128 -> 4x4x128
#         x = self.relu(self.conv4(x))             # 4x4x256

#         # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
#         x = x.view(x.size(0), -1)  # Flatten: batch_size x (4*4*256)

#         # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
#         x = self.relu(self.fc1(x))
#         x = self.dropout(x)
#         x = self.relu(self.fc2(x))
#         x = self.dropout(x)
#         x = self.fc3(x)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ª–æ–≥–∏—Ç—ã)

#         return x

# ============================================================================
# 3. –§–£–ù–ö–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø –ò –û–¶–ï–ù–ö–ò
# ============================================================================


def validate_model(model, val_loader, criterion, device):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ)
    """
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_val_loss = val_loss / len(val_loader)
    val_accuracy = 100 * correct / total

    return avg_val_loss, val_accuracy


def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫
    """
    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    # –î–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ (early stopping)
    best_val_accuracy = 0.0
    patience = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    epochs_without_improvement = 0

    for epoch in range(num_epochs):
        # ====================================================================
        # –û–ë–£–ß–ï–ù–ò–ï
        # ====================================================================
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (inputs, labels) in enumerate(train_loader):
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs, labels = inputs.to(device), labels.to(device)

            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            optimizer.zero_grad()

            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            loss.backward()
            optimizer.step()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É
        train_loss = running_loss / len(train_loader)
        train_accuracy = 100 * correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # ====================================================================
        # –í–ê–õ–ò–î–ê–¶–ò–Ø
        # ====================================================================
        val_loss, val_accuracy = validate_model(
            model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # –ü–µ—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫ —ç–ø–æ—Ö–∏
        print(f'–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}]:')
        print(
            f'  –û–±—É—á–µ–Ω–∏–µ  - –ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {train_accuracy:.2f}%')
        print(
            f'  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ü–æ—Ç–µ—Ä—è: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%')
        print('-' * 60)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ (–¥–ª—è early stopping)
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_without_improvement = 0
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            print(
                f'  ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%')
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print(f'  ‚èπÔ∏è –†–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤: {patience} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è')
                break

        print()

    return {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'best_val_accuracy': best_val_accuracy
    }


def evaluate_model_with_metrics(model, test_loader, device, classes):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    model.eval()

    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_predictions = []
    all_labels = []
    all_softmax_probs = []
    all_logits = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ (—Å—ã—Ä—ã—Ö –≤—ã—Ö–æ–¥–æ–≤)
            logits = model(inputs)

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            softmax_probs = torch.softmax(logits, dim=1)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            _, predicted = torch.max(logits, 1)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_softmax_probs.extend(softmax_probs.cpu().numpy())
            all_logits.extend(logits.cpu().numpy())

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy –º–∞—Å—Å–∏–≤—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_softmax_probs = np.array(all_softmax_probs)
    all_logits = np.array(all_logits)

    return all_predictions, all_labels, all_softmax_probs, all_logits

# ============================================================================
# 4. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –í–´–ß–ò–°–õ–ï–ù–ò–Ø –ú–ï–¢–†–ò–ö
# ============================================================================


def calculate_confidence_scores(softmax_probs):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ confidence scores –∫–∞–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    """
    confidence_scores = np.max(softmax_probs, axis=1)
    return confidence_scores


def calculate_top_k_accuracy(softmax_probs, true_labels, k=5):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Top-K accuracy
    """
    acc = top_k_accuracy_score(true_labels, softmax_probs, k=k)
    return acc * 100.0


def calculate_entropy(softmax_probs):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    H = -‚àë p_i * log(p_i)
    """
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–∞–ª–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)
    ent = scipy_entropy(softmax_probs, axis=1)
    return ent


def calculate_calibration_error(softmax_probs, predictions, true_labels, n_bins=10):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Expected Calibration Error (ECE)
    –ò–∑–º–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
    """
    probs_t = torch.from_numpy(softmax_probs).float()      # shape: [N, C]
    targets_t = torch.from_numpy(true_labels).long()       # shape: [N]

    # ECE —Å L1-–Ω–æ—Ä–º–æ–π (|conf - acc|), –∫–∞–∫ —É —Ç–µ–±—è –±—ã–ª–æ
    ece_metric = MulticlassCalibrationError(
        num_classes=softmax_probs.shape[1],
        n_bins=n_bins,
        norm='l1'
    )
    ece = float(ece_metric(probs_t, targets_t).item())

    # bin_data –Ω–µ –Ω—É–∂–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
    return ece, []

# ============================================================================
# 5. –§–£–ù–ö–¶–ò–ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò
# ============================================================================


def plot_training_history(training_history):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Å –ª–µ–Ω—Ç–∞–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞)
    """
    import numpy as np
    import matplotlib.pyplot as plt

    # --- –µ–¥–∏–Ω—ã–π —Å—Ç–∏–ª—å
    FIGSIZE = (12, 5)
    GRID_ALPHA = 0.3
    BAND_ALPHA = 0.15
    LINEWIDTH = 2

    train_accuracies = np.asarray(
        training_history['train_accuracies'], dtype=float)
    val_accuracies = np.asarray(
        training_history['val_accuracies'],   dtype=float)
    train_losses = np.asarray(
        training_history['train_losses'],     dtype=float)
    val_losses = np.asarray(training_history['val_losses'],       dtype=float)
    epochs = np.arange(1, len(train_accuracies) + 1)

    def _smooth(x, k=3):
        if len(x) < k:
            return x
        w = np.ones(k)/k
        y = np.convolve(x, w, mode='valid')
        pad = (len(x) - len(y)) // 2
        return np.pad(y, (pad, len(x)-len(y)-pad), mode='edge')

    def _roll_minmax(x, k=5):
        if k < 2 or len(x) < k:
            return x, x
        from collections import deque
        dmin, dmax, qmin, qmax, xs = [], [], deque(), deque(), x.tolist()
        for i, v in enumerate(xs):
            while qmin and xs[qmin[-1]] >= v:
                qmin.pop()
            qmin.append(i)
            while qmax and xs[qmax[-1]] <= v:
                qmax.pop()
            qmax.append(i)
            if qmin[0] <= i-k:
                qmin.popleft()
            if qmax[0] <= i-k:
                qmax.popleft()
            dmin.append(xs[qmin[0]])
            dmax.append(xs[qmax[0]])
        return np.array(dmin), np.array(dmax)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)

    # --- –¢–æ—á–Ω–æ—Å—Ç—å
    tr_s = _smooth(train_accuracies, k=3)
    va_s = _smooth(val_accuracies,   k=3)
    ax1.plot(epochs, tr_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')
    ax1.plot(epochs, va_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
    tr_lo, tr_hi = _roll_minmax(train_accuracies, k=5)
    va_lo, va_hi = _roll_minmax(val_accuracies,   k=5)
    ax1.fill_between(epochs, tr_lo, tr_hi, alpha=BAND_ALPHA)
    ax1.fill_between(epochs, va_lo, va_hi, alpha=BAND_ALPHA)
    ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    ax1.grid(True, alpha=GRID_ALPHA)
    ax1.legend(loc='lower right')

    # --- –ü–æ—Ç–µ—Ä–∏
    tl_s = _smooth(train_losses, k=3)
    vl_s = _smooth(val_losses,   k=3)
    ax2.plot(epochs, tl_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')
    ax2.plot(epochs, vl_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
    tl_lo, tl_hi = _roll_minmax(train_losses, k=5)
    vl_lo, vl_hi = _roll_minmax(val_losses,   k=5)
    ax2.fill_between(epochs, tl_lo, tl_hi, alpha=BAND_ALPHA)
    ax2.fill_between(epochs, vl_lo, vl_hi, alpha=BAND_ALPHA)
    ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('–ü–æ—Ç–µ—Ä—è')
    ax2.grid(True, alpha=GRID_ALPHA)
    ax2.legend(loc='upper right')

    plt.tight_layout()
    plt.show()

    # --- –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    print("\nüîç –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:")
    final_train_acc = float(train_accuracies[-1])
    final_val_acc = float(val_accuracies[-1])
    gap = final_train_acc - final_val_acc
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {final_train_acc:.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_acc:.2f}%")
    print(f"–†–∞–∑—Ä—ã–≤ (Train - Val): {gap:.2f}%")
    if gap < 3:
        print("‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç")
    elif gap < 8:
        print("‚ö†Ô∏è –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    elif gap < 15:
        print("üî∂ –£–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    else:
        print("‚ùå –°–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    return gap


def plot_confusion_matrix(true_labels, predictions, classes):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (–ø—Ä–æ—Ü–µ–Ω—Ç—ã + –∞–±—Å–æ–ª—é—Ç—ã, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ç–∏–Ω–µ)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix

    FIGSIZE = (8, 6)
    GRID_ALPHA = 0.3

    cm_abs = confusion_matrix(true_labels, predictions)
    with np.errstate(invalid='ignore'):
        cm = cm_abs / cm_abs.sum(axis=1, keepdims=True)
    cm = np.nan_to_num(cm)

    plt.figure(figsize=FIGSIZE)
    ax = sns.heatmap(cm, annot=False, cmap='Blues',
                     xticklabels=classes, yticklabels=classes,
                     vmin=0.0, vmax=1.0, cbar_kws={"label": "–î–æ–ª—è –ø–æ –∏—Å—Ç–∏–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É"})
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            txt = f"{cm[i, j]*100:.1f}%\n({cm_abs[i, j]})"
            ax.text(j+0.5, i+0.5, txt, ha='center',
                    va='center', fontsize=9, color='black')

    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã (–æ—à–∏–±–æ–∫)')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.grid(False)  # —É heatmap —Å–≤–æ—è —Å–µ—Ç–∫–∞
    plt.tight_layout()
    plt.show()


# ============================================================================
# 6. –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø - –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================================================


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏
    """
    print("=" * 80)
    print("–ü–†–ê–ö–¢–ò–ö–ê: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ò –ú–ï–¢–†–ò–ö–ò –ì–õ–£–ë–û–ö–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö CIFAR-10...")
    train_loader, val_loader, test_loader, classes = load_cifar10_data(
        batch_size=128, val_split=0.1)
    print(f"–ö–ª–∞—Å—Å—ã: {classes}")

    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = SimpleCNN_CIFAR(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {total_params:,}")

    # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")
    print("=" * 60)
    training_history = train_model(
        model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10
    )

    print(f"\nüéØ –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø:")
    print(
        f"–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {training_history['best_val_accuracy']:.2f}%")

    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    print("\n4. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")
    print("‚ö†Ô∏è  –í–ê–ñ–ù–û: –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏!")
    print("üîí –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    predictions, true_labels, softmax_probs, logits = evaluate_model_with_metrics(
        model, test_loader, device, classes
    )

    # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    test_accuracy = accuracy_score(true_labels, predictions) * 100
    val_accuracy = training_history['best_val_accuracy']

    print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –í–ê–õ–ò–î–ê–¶–ò–ò –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:   {test_accuracy:.2f}%")

    accuracy_diff = abs(val_accuracy - test_accuracy)
    if accuracy_diff < 2:
        print(f"‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞: {accuracy_diff:.2f}%)")
    elif accuracy_diff < 5:
        print(f"‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞: {accuracy_diff:.2f}%)")
    elif accuracy_diff < 10:
        print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞: {accuracy_diff:.2f}%)")
    else:
        print(f"‚ùå –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (—Ä–∞–∑–Ω–∏—Ü–∞: {accuracy_diff:.2f}%)")
        print("   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, —Ä–∞–∑–ª–∏—á–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")

    # ========================================================================
    # 5. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ò –ê–ù–ê–õ–ò–ó –ú–ï–¢–†–ò–ö
    # ========================================================================

    print("\n" + "=" * 50)
    print("–§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï")
    print("=" * 50)
    print("üî¨ –í—Å–µ –Ω–∏–∂–µ–ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–º —Ç–µ—Å—Ç —Å–µ—Ç–µ")

    # 5.1 –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Top-1 Accuracy)
    basic_accuracy = accuracy_score(true_labels, predictions) * 100
    print(f"\nüìä –ë–ê–ó–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"Top-1 Accuracy: {basic_accuracy:.2f}%")

    # 5.2 Confidence Scores
    confidence_scores = calculate_confidence_scores(softmax_probs)
    avg_confidence = np.mean(confidence_scores)
    print(f"–°—Ä–µ–¥–Ω–∏–π Confidence Score: {avg_confidence:.3f}")
    print(f"–ú–∏–Ω. Confidence Score: {np.min(confidence_scores):.3f}")
    print(f"–ú–∞–∫—Å. Confidence Score: {np.max(confidence_scores):.3f}")

    # 5.3 Top-K Accuracy
    print(f"\nüéØ TOP-K ACCURACY:")
    for k in [1, 3, 5]:
        top_k_acc = calculate_top_k_accuracy(softmax_probs, true_labels, k=k)
        print(f"Top-{k} Accuracy: {top_k_acc:.2f}%")

    # 5.4 –≠–Ω—Ç—Ä–æ–ø–∏—è
    entropy_values = calculate_entropy(softmax_probs)
    avg_entropy = np.mean(entropy_values)
    print(f"\nüîÄ –≠–ù–¢–†–û–ü–ò–Ø (–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å):")
    print(f"–°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {avg_entropy:.3f}")
    print(f"–ú–∏–Ω. —ç–Ω—Ç—Ä–æ–ø–∏—è: {np.min(entropy_values):.3f}")
    print(f"–ú–∞–∫—Å. —ç–Ω—Ç—Ä–æ–ø–∏—è: {np.max(entropy_values):.3f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(entropy_values):.3f}")

    # 5.5 –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞
    ece, bin_data = calculate_calibration_error(
        softmax_probs, predictions, true_labels)
    print(f"\n‚öñÔ∏è –ö–ê–õ–ò–ë–†–û–í–ö–ê:")
    print(
        f"Expected Calibration Error (ECE) –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –∑–∞—è–≤–ª–µ–Ω–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—Å–µ–≥–æ: {ece:.3f}")
    print(f"–ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–∏—Ç–∏—Ä–æ–≤–∞—Ç—å: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç ¬´—è —É–≤–µ—Ä–µ–Ω–∞ –Ω–∞ 80%¬ª, —Ç–æ –æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 76‚Äì80% —Å–ª—É—á–∞–µ–≤ –ø—Ä–∞–≤–∞ (–ø–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ).")
    print("–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ECE:")
    print("  - 0.0-0.05: –û—Ç–ª–∏—á–Ω–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞")
    print("  - 0.05-0.1: –•–æ—Ä–æ—à–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞")
    print("  - 0.1-0.2: –£–º–µ—Ä–µ–Ω–Ω–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞")
    print("  - >0.2: –ü–ª–æ—Ö–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–∞")

    # 5.6 –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    class_report = classification_report(true_labels, predictions,
                                         target_names=classes, digits=3)
    print(class_report)

    # ========================================================================
    # 6. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    # ========================================================================

    print("\n" + "=" * 50)
    print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 50)

    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    overfitting_gap = plot_training_history(training_history)

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    plot_confusion_matrix(true_labels, predictions, classes)

# ============================================================================
# 8. –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
# ============================================================================


if __name__ == "__main__":
    main()
