"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞
–ú–µ—Ç—Ä–∏–∫–∏: AnswerRelevancy, ToolCorrectness, TaskCompletion
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from deepeval_custom_llm import create_proxy_model, ProxyLLM  # noqa: E402
from agent_connector import AgentConnector  # noqa: E402
from dataset_parser import DatasetParser  # noqa: E402
from deepeval.metrics import (
    AnswerRelevancyMetric,
    ToolCorrectnessMetric,
    TaskCompletionMetric
)  # noqa: E402
from deepeval import evaluate  # noqa: E402
from deepeval.test_case import LLMTestCase, ToolCall  # noqa: E402
from typing import List, Union, Optional   # noqa: E402
import time  # noqa: E402


def convert_tools_to_toolcalls(tools_list: List[str]) -> List[ToolCall]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ç—É–ª–æ–≤ –≤ ToolCall –æ–±—ä–µ–∫—Ç—ã"""
    return [ToolCall(name=tool_name) for tool_name in tools_list]


def evaluate_agent(
    excel_path: str,
    agent_connector: AgentConnector,
    model: Union[str, ProxyLLM],
    urls: Optional[List[str]] = None,
    threshold: float = 0.7,
    sleep_time: float = 0.5
):
    """
    –û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ –ø–æ —Ç—Ä–µ–º –º–µ—Ç—Ä–∏–∫–∞–º
    """

    print("=" * 70)
    print("üöÄ –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ô –¢–ï–°–¢ –ê–ì–ï–ù–¢–ê")
    print("=" * 70)

    # ============= –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ =============
    print("\nüìÇ –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    parser = DatasetParser()
    df = parser.load_dataset(excel_path)

    if df is None:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é
    parser.preview_dataset(df, n=2)

    # ============= –®–ê–ì 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö =============
    print("\nüìù –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    pairs = parser.get_question_response_pairs(df)
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(pairs)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç-—Ç—É–ª—ã")

    if urls:
        print(f"üîó URLs –¥–ª—è –∞–≥–µ–Ω—Ç–∞: {urls}")

    # ============= –®–ê–ì 3: –ó–∞–ø—Ä–æ—Å—ã –∫ –∞–≥–µ–Ω—Ç—É =============
    print("\nü§ñ –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–∞...")

    test_cases = []

    for i, pair in enumerate(pairs, 1):
        question = pair['question']
        expected_response = pair['expected_response']
        expected_tools = pair['expected_tools']

        print(f"\n[{i}/{len(pairs)}] {question[:60]}...")

        # –ó–∞–ø—Ä–æ—Å –∫ –∞–≥–µ–Ω—Ç—É
        response = agent_connector.query(question, urls=urls)

        if response.get('error'):
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {response['error']}")
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        answer = response.get('output', '')
        tools_used = response.get('tools_used', [])

        print(f"   –û—Ç–≤–µ—Ç: {answer}")
        print(f"   –¢—É–ª—ã: {tools_used}")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç –∫–µ–π—Å
        test_case = LLMTestCase(
            input=question,
            actual_output=answer,
            expected_output=expected_response if expected_response else None,
            tools_called=convert_tools_to_toolcalls(tools_used),
            expected_tools=convert_tools_to_toolcalls(expected_tools)
        )

        test_cases.append(test_case)

        if i < len(pairs):
            time.sleep(sleep_time)

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç –∫–µ–π—Å–æ–≤")

    # ============= –®–ê–ì 4: –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ =============
    print("\nüìä –®–∞–≥ 4: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫...")

    metrics = [
        AnswerRelevancyMetric(
            threshold=threshold,
            model=model,
            include_reason=True
        ),
        ToolCorrectnessMetric(
            threshold=threshold,
            model=model,
            include_reason=True,
            should_exact_match=True,  # —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            should_consider_ordering=True  # —É—á–∏—Ç—ã–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –≤—ã–∑–æ–≤–∞ —Ç—É–ª–æ–≤
        ),
        TaskCompletionMetric(
            threshold=threshold,
            model=model,
            include_reason=True
        )
    ]

    print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏:")
    print("   ‚Ä¢ AnswerRelevancyMetric")
    print("   ‚Ä¢ ToolCorrectnessMetric")
    print("   ‚Ä¢ TaskCompletionMetric")

    # ============= –®–ê–ì 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ =============
    print("\nüß™ –®–∞–≥ 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏...")
    print("=" * 70)

    try:
        evaluate(
            test_cases=test_cases,
            metrics=metrics
        )

        print("\n" + "=" * 70)
        print("‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 70)
        print("\nüåê –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: https://app.confident-ai.com")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":

    # ============= –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =============

    # –ê–≥–µ–Ω—Ç
    agent = AgentConnector(
        endpoint_url="http://5.11.83.110:8004/ask",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        user_id="AleksM",
        session_id="3bb76ef7-3c21-4644-9890-eb5d6a223017"
    )

    # –ú–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫
    proxy_model = create_proxy_model(
        model="gpt-4o-mini",
        api_key="sk-proxy-maEZp5Yp0-h9nOHDZXtoZPql5VRW3CqTqakKOQgsQtQ",
        base_url="http://5.11.83.110:8000"
    )

    # –î–∞—Ç–∞—Å–µ—Ç
    excel_path = "data/evaluation_dataset.xlsx"

    # URLs –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∞–≥–µ–Ω—Ç—É
    test_urls = [
        "https://www.rentalads.com/apartments-for-rent/ny/new-york/"
    ]

    # ============= –ó–ê–ü–£–°–ö =============

    evaluate_agent(
        excel_path=excel_path,
        agent_connector=agent,
        model=proxy_model,  # –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "gpt-4o-mini"
        urls=test_urls,
        threshold=0.7,
        sleep_time=0.5
    )
