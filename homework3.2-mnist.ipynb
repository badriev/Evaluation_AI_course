{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ MNIST: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score, accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from torchmetrics.classification import MulticlassCalibrationError\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(batch_size=128, val_split=0.1):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ train/validation/test\n",
    "\n",
    "    MNIST —Å–æ–¥–µ—Ä–∂–∏—Ç 70,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä 28x28 –ø–∏–∫—Å–µ–ª–µ–π –≤ –æ—Ç—Ç–µ–Ω–∫–∞—Ö —Å–µ—Ä–æ–≥–æ:\n",
    "    - —Ü–∏—Ñ—Ä—ã –æ—Ç 0 –¥–æ 9 (10 –∫–ª–∞—Å—Å–æ–≤)\n",
    "    - 1 –∫–∞–Ω–∞–ª —Ü–≤–µ—Ç–∞ (grayscale), –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç CIFAR-10 —Å 3 –∫–∞–Ω–∞–ª–∞–º–∏\n",
    "\n",
    "    Args:\n",
    "        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ —Ä–∞–∑)\n",
    "        val_split: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ (0.1 = 10%)\n",
    "    \"\"\"\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)\n",
    "    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è - —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—É—Ç–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π\n",
    "    # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    transform_train = transforms.Compose([\n",
    "        # RandomRotation: —Å–ª—É—á–∞–π–Ω–æ –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ü–∏—Ñ—Ä)\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        # RandomAffine: —Å–ª—É—á–∞–π–Ω–æ —Å–¥–≤–∏–≥–∞–µ—Ç –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é\n",
    "        # –°—Ä–µ–¥–Ω–µ–µ –∏ std –¥–ª—è MNIST –≤—ã—á–∏—Å–ª–µ–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)\n",
    "    # –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–µ –º—ã —Ö–æ—Ç–∏–º –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ \"—Ä–µ–∞–ª—å–Ω—ã—Ö\" –¥–∞–Ω–Ω—ã—Ö\n",
    "    transform_val_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö MNIST\n",
    "    full_train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform_val_test\n",
    "    )\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞ train –∏ validation\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω—É–∂–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "    train_size = int((1 - val_split) * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    train_dataset, val_dataset_temp = torch.utils.data.random_split(\n",
    "        full_train_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "    # –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–∂–µ—Ç –∏—Å–∫–∞–∂–∞—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    val_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=False, transform=transform_val_test\n",
    "    )\n",
    "\n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "    val_indices = val_dataset_temp.indices\n",
    "    val_dataset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (DataLoader)\n",
    "    # DataLoader —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–∏–º\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ MNIST (—Ü–∏—Ñ—Ä—ã –æ—Ç 0 –¥–æ 9)\n",
    "    classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "    print(f\"–†–∞–∑–º–µ—Ä—ã –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "    print(f\"  - –û–±—É—á–∞—é—â–∏–π: {len(train_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
    "    print(f\"  - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π: {len(val_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
    "    print(f\"  - –¢–µ—Å—Ç–æ–≤—ã–π: {len(test_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_MNIST(nn.Module):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ—Å—Ç–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ MNIST\n",
    "\n",
    "    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:\n",
    "    - 3 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è —Å —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∏–º—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤ (32 -> 64 -> 128)\n",
    "    - MaxPooling –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–∫–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    - 2 –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è –≤ –∫–æ–Ω—Ü–µ\n",
    "    - Dropout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "    - –ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≤ –æ–±—É—á–µ–Ω–∏–∏, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—â–Ω–æ–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN_MNIST, self).__init__()\n",
    "\n",
    "        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏\n",
    "        # Conv2d(–≤—Ö–æ–¥–Ω—ã–µ_–∫–∞–Ω–∞–ª—ã, –≤—ã—Ö–æ–¥–Ω—ã–µ_–∫–∞–Ω–∞–ª—ã, —Ä–∞–∑–º–µ—Ä_—è–¥—Ä–∞, padding)\n",
    "        # MNIST –∏–º–µ–µ—Ç 1 –∫–∞–Ω–∞–ª (grayscale), –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç CIFAR-10 —Å 3 –∫–∞–Ω–∞–ª–∞–º–∏\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)     # 28x28x1 -> 28x28x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # 28x28x32 -> 28x28x64\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # 14x14x64 -> 14x14x128\n",
    "\n",
    "        # –ü—É–ª–∏–Ω–≥ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ 2 —Ä–∞–∑–∞\n",
    "        self.relu = nn.ReLU()           # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU: max(0, x)\n",
    "        self.dropout = nn.Dropout(0.5) # –°–ª—É—á–∞–π–Ω–æ \"–≤—ã–∫–ª—é—á–∞–µ—Ç\" 50% –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
    "\n",
    "        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏\n",
    "        # –ü–æ—Å–ª–µ 3 –ø—É–ª–∏–Ω–≥–æ–≤: 28 -> 14 -> 7 -> 3.5 (–æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ 3)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # 128 –∫–∞–Ω–∞–ª–æ–≤ * 3x3 = 1152 -> 256\n",
    "        self.fc2 = nn.Linear(256, 64)           # 256 -> 64\n",
    "        self.fc3 = nn.Linear(64, num_classes)   # 64 -> 10 –∫–ª–∞—Å—Å–æ–≤\n",
    "\n",
    "    def forward(self, x):\n",
    "        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –∏ –ø—É–ª–∏–Ω–≥–æ–º\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # 28x28x32 -> 14x14x32\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # 14x14x64 -> 7x7x64\n",
    "        x = self.pool(self.relu(self.conv3(x)))  # 7x7x128 -> 3x3x128\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤\n",
    "        x = x.view(x.size(0), -1)  # Flatten: batch_size x (128*3*3)\n",
    "\n",
    "        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ —Å dropout\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ª–æ–≥–∏—Ç—ã –¥–ª—è 10 –∫–ª–∞—Å—Å–æ–≤)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å Batch Normalization\n",
    "\n",
    "    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:\n",
    "    - Batch Normalization –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ)\n",
    "    - –ë–æ–ª—å—à–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö (64 -> 128 -> 256 -> 512)\n",
    "    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π\n",
    "    - –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "    - BatchNorm –ø–æ–º–æ–≥–∞–µ—Ç —Å internal covariate shift –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "\n",
    "        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å BatchNorm\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch Normalization\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # –ü—É–ª–∏–Ω–≥ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (–ø–æ—Å–ª–µ 4 –ø—É–ª–∏–Ω–≥–æ–≤: 28 -> 14 -> 7 -> 3 -> 1)\n",
    "        self.fc1 = nn.Linear(512 * 1 * 1, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å BatchNorm\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # 28x28x64 -> 14x14x64\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))  # 14x14x128 -> 7x7x128\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))  # 7x7x256 -> 3x3x256\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))  # 3x3x512 -> 1x1x512\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual –±–ª–æ–∫ –¥–ª—è ResNet-–ø–æ–¥–æ–±–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
    "\n",
    "    –û–°–û–ë–ï–ù–ù–û–°–¢–ò RESIDUAL CONNECTIONS:\n",
    "    - –ü—Ä–æ–ø—É—Å–∫–∞—é—Ç –≤—Ö–æ–¥ –Ω–∞–ø—Ä—è–º—É—é –∫ –≤—ã—Ö–æ–¥—É –±–ª–æ–∫–∞\n",
    "    - –ü–æ–º–æ–≥–∞—é—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å vanishing gradient –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö\n",
    "    - –ü–æ–∑–≤–æ–ª—è—é—Ç —Å—Ç—Ä–æ–∏—Ç—å —Å–µ—Ç–∏ —Å –¥–µ—Å—è—Ç–∫–∞–º–∏ —Å–ª–æ–µ–≤\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Residual connection (–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–≤—è–∑—å)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º 1x1 —Å–≤–µ—Ä—Ç–∫—É –¥–ª—è –ø–æ–¥–≥–æ–Ω–∫–∏\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Residual connection: F(x) + x\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetLike(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è MNIST\n",
    "\n",
    "    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:\n",
    "    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç residual –±–ª–æ–∫–∏ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    - –ú–æ–∂–µ—Ç –∏–º–µ—Ç—å –º–Ω–æ–≥–æ —Å–ª–æ–µ–≤ –±–µ–∑ degradation –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    - –õ—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "    - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –Ω–æ –¥–∞–µ—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNetLike, self).__init__()\n",
    "\n",
    "        # –ù–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–π\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Residual –±–ª–æ–∫–∏\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)    # 2 –±–ª–æ–∫–∞ –ø–æ 64 –∫–∞–Ω–∞–ª–∞\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 2 –±–ª–æ–∫–∞ –ø–æ 128 –∫–∞–Ω–∞–ª–æ–≤\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 2 –±–ª–æ–∫–∞ –ø–æ 256 –∫–∞–Ω–∞–ª–æ–≤\n",
    "\n",
    "        # –ü—É–ª–∏–Ω–≥ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π average pooling\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å residual –±–ª–æ–∫–æ–≤\"\"\"\n",
    "        layers = []\n",
    "        layers.append(ResNetBlock(in_channels, out_channels, stride))\n",
    "\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResNetBlock(out_channels, out_channels, stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # –ù–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–π\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # 28x28x64\n",
    "\n",
    "        # Residual –±–ª–æ–∫–∏\n",
    "        x = self.layer1(x)  # 28x28x64\n",
    "        x = self.layer2(x)  # 14x14x128\n",
    "        x = self.layer3(x)  # 7x7x256\n",
    "\n",
    "        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π pooling –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n",
    "        x = self.avgpool(x)  # 1x1x256\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫\n",
    "    \"\"\"\n",
    "    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # –î–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ (early stopping)\n",
    "    best_val_accuracy = 0.0\n",
    "    patience = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ====================================================================\n",
    "        # –û–ë–£–ß–ï–ù–ò–ï\n",
    "        # ====================================================================\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # ====================================================================\n",
    "        # –í–ê–õ–ò–î–ê–¶–ò–Ø\n",
    "        # ====================================================================\n",
    "        val_loss, val_accuracy = validate_model(\n",
    "            model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # –ü–µ—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫ —ç–ø–æ—Ö–∏\n",
    "        print(f'–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}]:')\n",
    "        print(\n",
    "            f'  –û–±—É—á–µ–Ω–∏–µ  - –ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {train_accuracy:.2f}%')\n",
    "        print(\n",
    "            f'  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ü–æ—Ç–µ—Ä—è: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ (–¥–ª—è early stopping)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "            print(\n",
    "                f'  ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'  ‚èπÔ∏è –†–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤: {patience} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è')\n",
    "                break\n",
    "\n",
    "        print()\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_accuracy': best_val_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_metrics(model, test_loader, device, classes):\n",
    "    \"\"\"\n",
    "    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_softmax_probs = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ (—Å—ã—Ä—ã—Ö –≤—ã—Ö–æ–¥–æ–≤)\n",
    "            logits = model(inputs)\n",
    "\n",
    "            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "            softmax_probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_softmax_probs.extend(softmax_probs.cpu().numpy())\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy –º–∞—Å—Å–∏–≤—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_softmax_probs = np.array(all_softmax_probs)\n",
    "    all_logits = np.array(all_logits)\n",
    "\n",
    "    return all_predictions, all_labels, all_softmax_probs, all_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_scores(softmax_probs):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ confidence scores –∫–∞–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "    \"\"\"\n",
    "    confidence_scores = np.max(softmax_probs, axis=1)\n",
    "    return confidence_scores\n",
    "\n",
    "\n",
    "def calculate_top_k_accuracy(softmax_probs, true_labels, k=5):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Top-K accuracy\n",
    "    \"\"\"\n",
    "    acc = top_k_accuracy_score(true_labels, softmax_probs, k=k)\n",
    "    return acc * 100.0\n",
    "\n",
    "\n",
    "def calculate_entropy(softmax_probs):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    H = -‚àë p_i * log(p_i)\n",
    "    \"\"\"\n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–∞–ª–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)\n",
    "    ent = scipy_entropy(softmax_probs, axis=1)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def calculate_calibration_error(softmax_probs, predictions, true_labels, n_bins=10):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Expected Calibration Error (ECE)\n",
    "    –ò–∑–º–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é\n",
    "    \"\"\"\n",
    "    probs_t = torch.from_numpy(softmax_probs).float()      # shape: [N, C]\n",
    "    targets_t = torch.from_numpy(true_labels).long()       # shape: [N]\n",
    "\n",
    "    # ECE —Å L1-–Ω–æ—Ä–º–æ–π (|conf - acc|), –∫–∞–∫ —É —Ç–µ–±—è –±—ã–ª–æ\n",
    "    ece_metric = MulticlassCalibrationError(\n",
    "        num_classes=softmax_probs.shape[1],\n",
    "        n_bins=n_bins,\n",
    "        norm='l1'\n",
    "    )\n",
    "    ece = float(ece_metric(probs_t, targets_t).item())\n",
    "\n",
    "    # bin_data –Ω–µ –Ω—É–∂–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å —Å–∏–≥–Ω–∞—Ç—É—Ä—ã\n",
    "    return ece, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –§—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(training_history):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Å –ª–µ–Ω—Ç–∞–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # --- –µ–¥–∏–Ω—ã–π —Å—Ç–∏–ª—å\n",
    "    FIGSIZE = (12, 5)\n",
    "    GRID_ALPHA = 0.3\n",
    "    BAND_ALPHA = 0.15\n",
    "    LINEWIDTH = 2\n",
    "\n",
    "    train_accuracies = np.asarray(\n",
    "        training_history['train_accuracies'], dtype=float)\n",
    "    val_accuracies = np.asarray(\n",
    "        training_history['val_accuracies'],   dtype=float)\n",
    "    train_losses = np.asarray(\n",
    "        training_history['train_losses'],     dtype=float)\n",
    "    val_losses = np.asarray(training_history['val_losses'],       dtype=float)\n",
    "    epochs = np.arange(1, len(train_accuracies) + 1)\n",
    "\n",
    "    def _smooth(x, k=3):\n",
    "        if len(x) < k:\n",
    "            return x\n",
    "        w = np.ones(k)/k\n",
    "        y = np.convolve(x, w, mode='valid')\n",
    "        pad = (len(x) - len(y)) // 2\n",
    "        return np.pad(y, (pad, len(x)-len(y)-pad), mode='edge')\n",
    "\n",
    "    def _roll_minmax(x, k=5):\n",
    "        if k < 2 or len(x) < k:\n",
    "            return x, x\n",
    "        from collections import deque\n",
    "        dmin, dmax, qmin, qmax, xs = [], [], deque(), deque(), x.tolist()\n",
    "        for i, v in enumerate(xs):\n",
    "            while qmin and xs[qmin[-1]] >= v:\n",
    "                qmin.pop()\n",
    "            qmin.append(i)\n",
    "            while qmax and xs[qmax[-1]] <= v:\n",
    "                qmax.pop()\n",
    "            qmax.append(i)\n",
    "            if qmin[0] <= i-k:\n",
    "                qmin.popleft()\n",
    "            if qmax[0] <= i-k:\n",
    "                qmax.popleft()\n",
    "            dmin.append(xs[qmin[0]])\n",
    "            dmax.append(xs[qmax[0]])\n",
    "        return np.array(dmin), np.array(dmax)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)\n",
    "\n",
    "    # --- –¢–æ—á–Ω–æ—Å—Ç—å\n",
    "    tr_s = _smooth(train_accuracies, k=3)\n",
    "    va_s = _smooth(val_accuracies,   k=3)\n",
    "    ax1.plot(epochs, tr_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')\n",
    "    ax1.plot(epochs, va_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')\n",
    "    tr_lo, tr_hi = _roll_minmax(train_accuracies, k=5)\n",
    "    va_lo, va_hi = _roll_minmax(val_accuracies,   k=5)\n",
    "    ax1.fill_between(epochs, tr_lo, tr_hi, alpha=BAND_ALPHA)\n",
    "    ax1.fill_between(epochs, va_lo, va_hi, alpha=BAND_ALPHA)\n",
    "    ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º')\n",
    "    ax1.set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "    ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')\n",
    "    ax1.grid(True, alpha=GRID_ALPHA)\n",
    "    ax1.legend(loc='lower right')\n",
    "\n",
    "    # --- –ü–æ—Ç–µ—Ä–∏\n",
    "    tl_s = _smooth(train_losses, k=3)\n",
    "    vl_s = _smooth(val_losses,   k=3)\n",
    "    ax2.plot(epochs, tl_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')\n",
    "    ax2.plot(epochs, va_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')\n",
    "    tl_lo, tl_hi = _roll_minmax(train_losses, k=5)\n",
    "    vl_lo, vl_hi = _roll_minmax(val_losses,   k=5)\n",
    "    ax2.fill_between(epochs, tl_lo, tl_hi, alpha=BAND_ALPHA)\n",
    "    ax2.fill_between(epochs, vl_lo, vl_hi, alpha=BAND_ALPHA)\n",
    "    ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')\n",
    "    ax2.set_xlabel('–≠–ø–æ—Ö–∞')\n",
    "    ax2.set_ylabel('–ü–æ—Ç–µ—Ä—è')\n",
    "    ax2.grid(True, alpha=GRID_ALPHA)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "    print(\"\\nüîç –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:\")\n",
    "    final_train_acc = float(train_accuracies[-1])\n",
    "    final_val_acc = float(val_accuracies[-1])\n",
    "    gap = final_train_acc - final_val_acc\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {final_train_acc:.2f}%\")\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_acc:.2f}%\")\n",
    "    print(f\"–†–∞–∑—Ä—ã–≤ (Train - Val): {gap:.2f}%\")\n",
    "    if gap < 3:\n",
    "        print(\"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç\")\n",
    "    elif gap < 8:\n",
    "        print(\"‚ö†Ô∏è –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
    "    elif gap < 15:\n",
    "        print(\"üî∂ –£–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
    "    else:\n",
    "        print(\"‚ùå –°–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
    "    return gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, predictions, classes):\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (–ø—Ä–æ—Ü–µ–Ω—Ç—ã + –∞–±—Å–æ–ª—é—Ç—ã, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ç–∏–Ω–µ)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    FIGSIZE = (8, 6)\n",
    "    GRID_ALPHA = 0.3\n",
    "\n",
    "    cm_abs = confusion_matrix(true_labels, predictions)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        cm = cm_abs / cm_abs.sum(axis=1, keepdims=True)\n",
    "    cm = np.nan_to_num(cm)\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    ax = sns.heatmap(cm, annot=False, cmap='Blues',\n",
    "                     xticklabels=classes, yticklabels=classes,\n",
    "                     vmin=0.0, vmax=1.0, cbar_kws={\"label\": \"–î–æ–ª—è –ø–æ –∏—Å—Ç–∏–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É\"})\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            txt = f\"{cm[i, j]*100:.1f}%\\n({cm_abs[i, j]})\"\n",
    "            ax.text(j+0.5, i+0.5, txt, ha='center',\n",
    "                    va='center', fontsize=9, color='black')\n",
    "\n",
    "    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã (–æ—à–∏–±–æ–∫)')\n",
    "    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\n",
    "    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')\n",
    "    plt.grid(False)  # —É heatmap —Å–≤–æ—è —Å–µ—Ç–∫–∞\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"–ì–õ–£–ë–û–ö–û–ï –û–ë–£–ß–ï–ù–ò–ï –ù–ê MNIST: –°–†–ê–í–ù–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST\n",
    "    print(\"\\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST...\")\n",
    "    train_loader, val_loader, test_loader, classes = load_mnist_data(\n",
    "        batch_size=128, val_split=0.1)\n",
    "    print(f\"–ö–ª–∞—Å—Å—ã: {classes}\")\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    models_config = [\n",
    "        {\n",
    "            'name': 'SimpleCNN_MNIST',\n",
    "            'model': SimpleCNN_MNIST(num_classes=10),\n",
    "            'description': '–ü—Ä–æ—Å—Ç–∞—è CNN —Å 3 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏ —Å–ª–æ—è–º–∏'\n",
    "        },\n",
    "        {\n",
    "            'name': 'AdvancedCNN',\n",
    "            'model': AdvancedCNN(num_classes=10),\n",
    "            'description': '–£–ª—É—á—à–µ–Ω–Ω–∞—è CNN —Å BatchNorm –∏ 4 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏ —Å–ª–æ—è–º–∏'\n",
    "        },\n",
    "        {\n",
    "            'name': 'ResNetLike',\n",
    "            'model': ResNetLike(num_classes=10),\n",
    "            'description': 'ResNet-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å residual –±–ª–æ–∫–∞–º–∏'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n",
    "    all_results = {}\n",
    "\n",
    "    # 2. –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
    "    for i, config in enumerate(models_config, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"2.{i} –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò: {config['name']}\")\n",
    "        print(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {config['description']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        model = config['model'].to(device)\n",
    "\n",
    "        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {total_params:,}\")\n",
    "\n",
    "        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        criterion = nn.CrossEntropyLoss()  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "\n",
    "        # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "        start_time = time.time()\n",
    "\n",
    "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        training_history = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10\n",
    "        )\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "        print(f\"\\n–û—Ü–µ–Ω–∫–∞ {config['name']} –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...\")\n",
    "        predictions, true_labels, softmax_probs, logits = evaluate_model_with_metrics(\n",
    "            model, test_loader, device, classes\n",
    "        )\n",
    "\n",
    "        # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "        test_accuracy = accuracy_score(true_labels, predictions) * 100\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        all_results[config['name']] = {\n",
    "            'model': model,\n",
    "            'training_history': training_history,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'predictions': predictions,\n",
    "            'true_labels': true_labels,\n",
    "            'softmax_probs': softmax_probs,\n",
    "            'training_time': training_time,\n",
    "            'total_params': total_params\n",
    "        }\n",
    "\n",
    "        print(f\"üéØ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å {config['name']}: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # 3. –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"3. –°–†–ê–í–ù–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    print(\"\\nüìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'–ú–æ–¥–µ–ª—å':<15} {'–ü–∞—Ä–∞–º–µ—Ç—Ä—ã':<12} {'–í—Ä–µ–º—è':<8} {'–í–∞–ª.':<6} {'–¢–µ—Å—Ç':<6}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    best_model_name = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for name, results in all_results.items():\n",
    "        params = results['total_params']\n",
    "        train_time = results['training_time']\n",
    "        val_acc = results['training_history']['best_val_accuracy']\n",
    "        test_acc = results['test_accuracy']\n",
    "\n",
    "        print(f\"{name:<15} {params:<12,} {train_time:<8.1f} {val_acc:<6.1f} {test_acc:<6.1f}\")\n",
    "\n",
    "        if test_acc > best_accuracy:\n",
    "            best_accuracy = test_acc\n",
    "            best_model_name = name\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} (—Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%)\")\n",
    "\n",
    "    # 4. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"4. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_results = all_results[best_model_name]\n",
    "    predictions = best_results['predictions']\n",
    "    true_labels = best_results['true_labels']\n",
    "    softmax_probs = best_results['softmax_probs']\n",
    "\n",
    "    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    print(f\"\\nüìä –ë–ê–ó–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï:\")\n",
    "    print(f\"Top-1 Accuracy: {best_results['test_accuracy']:.2f}%\")\n",
    "\n",
    "    # Confidence Scores\n",
    "    confidence_scores = calculate_confidence_scores(softmax_probs)\n",
    "    avg_confidence = np.mean(confidence_scores)\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π Confidence Score: {avg_confidence:.3f}\")\n",
    "\n",
    "    # Top-K Accuracy\n",
    "    print(f\"\\nüéØ TOP-K ACCURACY:\")\n",
    "    for k in [1, 3, 5]:\n",
    "        top_k_acc = calculate_top_k_accuracy(softmax_probs, true_labels, k=k)\n",
    "        print(f\"Top-{k} Accuracy: {top_k_acc:.2f}%\")\n",
    "\n",
    "    # –≠–Ω—Ç—Ä–æ–ø–∏—è\n",
    "    entropy_values = calculate_entropy(softmax_probs)\n",
    "    avg_entropy = np.mean(entropy_values)\n",
    "    print(f\"\\nüîÄ –≠–ù–¢–†–û–ü–ò–Ø (–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å):\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {avg_entropy:.3f}\")\n",
    "\n",
    "    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞\n",
    "    ece, _ = calculate_calibration_error(\n",
    "        softmax_probs, predictions, true_labels)\n",
    "    print(f\"\\n‚öñÔ∏è –ö–ê–õ–ò–ë–†–û–í–ö–ê:\")\n",
    "    print(f\"Expected Calibration Error (ECE): {ece:.3f}\")\n",
    "\n",
    "    # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "    print(f\"\\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:\")\n",
    "    class_report = classification_report(true_labels, predictions,\n",
    "                                         target_names=classes, digits=3)\n",
    "    print(class_report)\n",
    "\n",
    "    # 5. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "    overfitting_gap = plot_training_history(best_results['training_history'])\n",
    "\n",
    "    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n",
    "    plot_confusion_matrix(true_labels, predictions, classes)\n",
    "\n",
    "    # 6. –í–´–í–û–î–´\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"–í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nüèÜ –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {best_model_name}\")\n",
    "    print(f\"   - –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%\")\n",
    "    print(f\"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {best_results['total_params']:,}\")\n",
    "    print(f\"   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {best_results['training_time']:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "    print(f\"\\nüìà –û–°–û–ë–ï–ù–ù–û–°–¢–ò –ö–ê–ñ–î–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:\")\n",
    "\n",
    "    for name, results in all_results.items():\n",
    "        test_acc = results['test_accuracy']\n",
    "        params = results['total_params']\n",
    "        train_time = results['training_time']\n",
    "\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"   - –¢–æ—á–Ω–æ—Å—Ç—å: {test_acc:.2f}%\")\n",
    "        print(f\"   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params:,}\")\n",
    "        print(f\"   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time:.2f} —Å–µ–∫—É–Ω–¥\")\n",
    "        if name == 'SimpleCNN_MNIST':\n",
    "            print(\"   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –±—ã—Å—Ç—Ä–∞—è –≤ –æ–±—É—á–µ–Ω–∏–∏\")\n",
    "            print(\"   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤\")\n",
    "        elif name == 'AdvancedCNN':\n",
    "            print(\"   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: BatchNorm –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –±–æ–ª—å—à–µ —Å–ª–æ–µ–≤\")\n",
    "            print(\"   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Ö–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ residual connections\")\n",
    "        elif name == 'ResNetLike':\n",
    "            print(\"   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: Residual –±–ª–æ–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ\")\n",
    "            print(\"   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\")\n",
    "\n",
    "    print(f\"\\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "    print(f\"   - –î–ª—è production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ {best_model_name}\")\n",
    "    print(\"   - –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –µ—â–µ –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
    "    print(\"   - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (SGD —Å momentum)\")\n",
    "    print(\"   - –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
