import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
from sklearn.metrics import top_k_accuracy_score, accuracy_score, classification_report, confusion_matrix
from scipy.stats import entropy as scipy_entropy
from torchmetrics.classification import MulticlassCalibrationError
import warnings
import time
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ============================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (MNIST)
# ============================================================================

def load_mnist_data(batch_size=128, val_split=0.1):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ MNIST —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ train/validation/test

    MNIST —Å–æ–¥–µ—Ä–∂–∏—Ç 70,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä 28x28 –ø–∏–∫—Å–µ–ª–µ–π –≤ –æ—Ç—Ç–µ–Ω–∫–∞—Ö —Å–µ—Ä–æ–≥–æ:
    - —Ü–∏—Ñ—Ä—ã –æ—Ç 0 –¥–æ 9 (10 –∫–ª–∞—Å—Å–æ–≤)
    - 1 –∫–∞–Ω–∞–ª —Ü–≤–µ—Ç–∞ (grayscale), –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç CIFAR-10 —Å 3 –∫–∞–Ω–∞–ª–∞–º–∏

    Args:
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ —Ä–∞–∑)
        val_split: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –æ—Ç –æ–±—É—á–∞—é—â–µ–≥–æ (0.1 = 10%)
    """

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è - —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—É—Ç–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    transform_train = transforms.Compose([
        # RandomRotation: —Å–ª—É—á–∞–π–Ω–æ –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ü–∏—Ñ—Ä)
        transforms.RandomRotation(degrees=10),
        # RandomAffine: —Å–ª—É—á–∞–π–Ω–æ —Å–¥–≤–∏–≥–∞–µ—Ç –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        transforms.ToTensor(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
        # –°—Ä–µ–¥–Ω–µ–µ –∏ std –¥–ª—è MNIST –≤—ã—á–∏—Å–ª–µ–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
    # –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–µ –º—ã —Ö–æ—Ç–∏–º –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ "—Ä–µ–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö
    transform_val_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö MNIST
    full_train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform_train
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform_val_test
    )

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –Ω–∞ train –∏ validation
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω—É–∂–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    train_size = int((1 - val_split) * len(full_train_dataset))
    val_size = len(full_train_dataset) - train_size

    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    generator = torch.Generator().manual_seed(42)
    train_dataset, val_dataset_temp = torch.utils.data.random_split(
        full_train_dataset, [train_size, val_size], generator=generator
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    # –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–∂–µ—Ç –∏—Å–∫–∞–∂–∞—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞
    val_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=False, transform=transform_val_test
    )

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    val_indices = val_dataset_temp.indices
    val_dataset = torch.utils.data.Subset(val_dataset, val_indices)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö (DataLoader)
    # DataLoader —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–∏–º
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ MNIST (—Ü–∏—Ñ—Ä—ã –æ—Ç 0 –¥–æ 9)
    classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')

    print(f"–†–∞–∑–º–µ—Ä—ã –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  - –û–±—É—á–∞—é—â–∏–π: {len(train_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π: {len(val_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  - –¢–µ—Å—Ç–æ–≤—ã–π: {len(test_dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")

    return train_loader, val_loader, test_loader, classes

# ============================================================================
# 2. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–† –ù–ï–ô–†–û–ù–ù–´–• –°–ï–¢–ï–ô
# ============================================================================

class SimpleCNN_MNIST(nn.Module):
    """
    –ü—Ä–æ—Å—Ç–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ MNIST

    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:
    - 3 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è —Å —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∏–º—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤ (32 -> 64 -> 128)
    - MaxPooling –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–∫–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    - 2 –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è –≤ –∫–æ–Ω—Ü–µ
    - Dropout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    - –ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≤ –æ–±—É—á–µ–Ω–∏–∏, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—â–Ω–æ–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
    """

    def __init__(self, num_classes=10):
        super(SimpleCNN_MNIST, self).__init__()

        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        # Conv2d(–≤—Ö–æ–¥–Ω—ã–µ_–∫–∞–Ω–∞–ª—ã, –≤—ã—Ö–æ–¥–Ω—ã–µ_–∫–∞–Ω–∞–ª—ã, —Ä–∞–∑–º–µ—Ä_—è–¥—Ä–∞, padding)
        # MNIST –∏–º–µ–µ—Ç 1 –∫–∞–Ω–∞–ª (grayscale), –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç CIFAR-10 —Å 3 –∫–∞–Ω–∞–ª–∞–º–∏
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)     # 28x28x1 -> 28x28x32
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)    # 28x28x32 -> 28x28x64
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)   # 14x14x64 -> 14x14x128

        # –ü—É–ª–∏–Ω–≥ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        self.pool = nn.MaxPool2d(2, 2)  # –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ 2 —Ä–∞–∑–∞
        self.relu = nn.ReLU()           # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU: max(0, x)
        self.dropout = nn.Dropout(0.5) # –°–ª—É—á–∞–π–Ω–æ "–≤—ã–∫–ª—é—á–∞–µ—Ç" 50% –Ω–µ–π—Ä–æ–Ω–æ–≤

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        # –ü–æ—Å–ª–µ 3 –ø—É–ª–∏–Ω–≥–æ–≤: 28 -> 14 -> 7 -> 3.5 (–æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ 3)
        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # 128 –∫–∞–Ω–∞–ª–æ–≤ * 3x3 = 1152 -> 256
        self.fc2 = nn.Linear(256, 64)           # 256 -> 64
        self.fc3 = nn.Linear(64, num_classes)   # 64 -> 10 –∫–ª–∞—Å—Å–æ–≤

    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –∏ –ø—É–ª–∏–Ω–≥–æ–º
        x = self.pool(self.relu(self.conv1(x)))  # 28x28x32 -> 14x14x32
        x = self.pool(self.relu(self.conv2(x)))  # 14x14x64 -> 7x7x64
        x = self.pool(self.relu(self.conv3(x)))  # 7x7x128 -> 3x3x128

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
        x = x.view(x.size(0), -1)  # Flatten: batch_size x (128*3*3)

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ —Å dropout
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ª–æ–≥–∏—Ç—ã –¥–ª—è 10 –∫–ª–∞—Å—Å–æ–≤)

        return x


class AdvancedCNN(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å Batch Normalization

    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:
    - Batch Normalization –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ)
    - –ë–æ–ª—å—à–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö (64 -> 128 -> 256 -> 512)
    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π
    - –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏–µ
    - BatchNorm –ø–æ–º–æ–≥–∞–µ—Ç —Å internal covariate shift –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate
    """

    def __init__(self, num_classes=10):
        super(AdvancedCNN, self).__init__()

        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å BatchNorm
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)  # Batch Normalization

        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)

        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)

        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)
        self.bn4 = nn.BatchNorm2d(512)

        # –ü—É–ª–∏–Ω–≥ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (–ø–æ—Å–ª–µ 4 –ø—É–ª–∏–Ω–≥–æ–≤: 28 -> 14 -> 7 -> 3 -> 1)
        self.fc1 = nn.Linear(512 * 1 * 1, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 64)
        self.fc4 = nn.Linear(64, num_classes)

    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å BatchNorm
        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # 28x28x64 -> 14x14x64
        x = self.pool(self.relu(self.bn2(self.conv2(x))))  # 14x14x128 -> 7x7x128
        x = self.pool(self.relu(self.bn3(self.conv3(x))))  # 7x7x256 -> 3x3x256
        x = self.pool(self.relu(self.bn4(self.conv4(x))))  # 3x3x512 -> 1x1x512

        # Flatten
        x = x.view(x.size(0), -1)

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)

        return x


class ResNetBlock(nn.Module):
    """
    Residual –±–ª–æ–∫ –¥–ª—è ResNet-–ø–æ–¥–æ–±–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

    –û–°–û–ë–ï–ù–ù–û–°–¢–ò RESIDUAL CONNECTIONS:
    - –ü—Ä–æ–ø—É—Å–∫–∞—é—Ç –≤—Ö–æ–¥ –Ω–∞–ø—Ä—è–º—É—é –∫ –≤—ã—Ö–æ–¥—É –±–ª–æ–∫–∞
    - –ü–æ–º–æ–≥–∞—é—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å vanishing gradient –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö
    - –ü–æ–∑–≤–æ–ª—è—é—Ç —Å—Ç—Ä–æ–∏—Ç—å —Å–µ—Ç–∏ —Å –¥–µ—Å—è—Ç–∫–∞–º–∏ —Å–ª–æ–µ–≤
    """

    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock, self).__init__()

        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Residual connection (–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–≤—è–∑—å)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º 1x1 —Å–≤–µ—Ä—Ç–∫—É –¥–ª—è –ø–æ–¥–≥–æ–Ω–∫–∏
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

        self.relu = nn.ReLU()

    def forward(self, x):
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        # Residual connection: F(x) + x
        out += self.shortcut(x)
        out = self.relu(out)

        return out


class ResNetLike(nn.Module):
    """
    ResNet-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è MNIST

    –û–°–û–ë–ï–ù–ù–û–°–¢–ò –≠–¢–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç residual –±–ª–æ–∫–∏ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    - –ú–æ–∂–µ—Ç –∏–º–µ—Ç—å –º–Ω–æ–≥–æ —Å–ª–æ–µ–≤ –±–µ–∑ degradation –∫–∞—á–µ—Å—Ç–≤–∞
    - –õ—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
    - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –Ω–æ –¥–∞–µ—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    """

    def __init__(self, num_classes=10):
        super(ResNetLike, self).__init__()

        # –ù–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–π
        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()

        # Residual –±–ª–æ–∫–∏
        self.layer1 = self._make_layer(64, 64, 2, stride=1)    # 2 –±–ª–æ–∫–∞ –ø–æ 64 –∫–∞–Ω–∞–ª–∞
        self.layer2 = self._make_layer(64, 128, 2, stride=2)   # 2 –±–ª–æ–∫–∞ –ø–æ 128 –∫–∞–Ω–∞–ª–æ–≤
        self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 2 –±–ª–æ–∫–∞ –ø–æ 256 –∫–∞–Ω–∞–ª–æ–≤

        # –ü—É–ª–∏–Ω–≥ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π average pooling
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å residual –±–ª–æ–∫–æ–≤"""
        layers = []
        layers.append(ResNetBlock(in_channels, out_channels, stride))

        for _ in range(1, num_blocks):
            layers.append(ResNetBlock(out_channels, out_channels, stride=1))

        return nn.Sequential(*layers)

    def forward(self, x):
        # –ù–∞—á–∞–ª—å–Ω—ã–π —Å–ª–æ–π
        x = self.relu(self.bn1(self.conv1(x)))  # 28x28x64

        # Residual –±–ª–æ–∫–∏
        x = self.layer1(x)  # 28x28x64
        x = self.layer2(x)  # 14x14x128
        x = self.layer3(x)  # 7x7x256

        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π pooling –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        x = self.avgpool(x)  # 1x1x256
        x = x.view(x.size(0), -1)  # Flatten
        x = self.dropout(x)
        x = self.fc(x)

        return x

# ============================================================================
# 3. –§–£–ù–ö–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø –ò –û–¶–ï–ù–ö–ò (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ============================================================================

def validate_model(model, val_loader, criterion, device):
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ)
    """
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_val_loss = val_loss / len(val_loader)
    val_accuracy = 100 * correct / total

    return avg_val_loss, val_accuracy


def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫
    """
    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    # –î–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ (early stopping)
    best_val_accuracy = 0.0
    patience = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    epochs_without_improvement = 0

    for epoch in range(num_epochs):
        # ====================================================================
        # –û–ë–£–ß–ï–ù–ò–ï
        # ====================================================================
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (inputs, labels) in enumerate(train_loader):
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs, labels = inputs.to(device), labels.to(device)

            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            optimizer.zero_grad()

            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            loss.backward()
            optimizer.step()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É
        train_loss = running_loss / len(train_loader)
        train_accuracy = 100 * correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # ====================================================================
        # –í–ê–õ–ò–î–ê–¶–ò–Ø
        # ====================================================================
        val_loss, val_accuracy = validate_model(
            model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # –ü–µ—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫ —ç–ø–æ—Ö–∏
        print(f'–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}]:')
        print(
            f'  –û–±—É—á–µ–Ω–∏–µ  - –ü–æ—Ç–µ—Ä—è: {train_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {train_accuracy:.2f}%')
        print(
            f'  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ü–æ—Ç–µ—Ä—è: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_accuracy:.2f}%')
        print('-' * 60)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ (–¥–ª—è early stopping)
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_without_improvement = 0
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            print(
                f'  ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%')
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print(f'  ‚èπÔ∏è –†–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤: {patience} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è')
                break

        print()

    return {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'best_val_accuracy': best_val_accuracy
    }


def evaluate_model_with_metrics(model, test_loader, device, classes):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    model.eval()

    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_predictions = []
    all_labels = []
    all_softmax_probs = []
    all_logits = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ (—Å—ã—Ä—ã—Ö –≤—ã—Ö–æ–¥–æ–≤)
            logits = model(inputs)

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            softmax_probs = torch.softmax(logits, dim=1)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            _, predicted = torch.max(logits, 1)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_softmax_probs.extend(softmax_probs.cpu().numpy())
            all_logits.extend(logits.cpu().numpy())

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy –º–∞—Å—Å–∏–≤—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_softmax_probs = np.array(all_softmax_probs)
    all_logits = np.array(all_logits)

    return all_predictions, all_labels, all_softmax_probs, all_logits

# ============================================================================
# 4. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –í–´–ß–ò–°–õ–ï–ù–ò–Ø –ú–ï–¢–†–ò–ö (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ============================================================================

def calculate_confidence_scores(softmax_probs):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ confidence scores –∫–∞–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    """
    confidence_scores = np.max(softmax_probs, axis=1)
    return confidence_scores


def calculate_top_k_accuracy(softmax_probs, true_labels, k=5):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Top-K accuracy
    """
    acc = top_k_accuracy_score(true_labels, softmax_probs, k=k)
    return acc * 100.0


def calculate_entropy(softmax_probs):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    H = -‚àë p_i * log(p_i)
    """
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–∞–ª–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)
    ent = scipy_entropy(softmax_probs, axis=1)
    return ent


def calculate_calibration_error(softmax_probs, predictions, true_labels, n_bins=10):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Expected Calibration Error (ECE)
    –ò–∑–º–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
    """
    probs_t = torch.from_numpy(softmax_probs).float()      # shape: [N, C]
    targets_t = torch.from_numpy(true_labels).long()       # shape: [N]

    # ECE —Å L1-–Ω–æ—Ä–º–æ–π (|conf - acc|), –∫–∞–∫ —É —Ç–µ–±—è –±—ã–ª–æ
    ece_metric = MulticlassCalibrationError(
        num_classes=softmax_probs.shape[1],
        n_bins=n_bins,
        norm='l1'
    )
    ece = float(ece_metric(probs_t, targets_t).item())

    # bin_data –Ω–µ –Ω—É–∂–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
    return ece, []

# ============================================================================
# 5. –§–£–ù–ö–¶–ò–ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò (–ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ============================================================================

def plot_training_history(training_history):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Å –ª–µ–Ω—Ç–∞–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    
    # --- –µ–¥–∏–Ω—ã–π —Å—Ç–∏–ª—å
    FIGSIZE = (12, 5)
    GRID_ALPHA = 0.3
    BAND_ALPHA = 0.15
    LINEWIDTH = 2

    train_accuracies = np.asarray(
        training_history['train_accuracies'], dtype=float)
    val_accuracies = np.asarray(
        training_history['val_accuracies'],   dtype=float)
    train_losses = np.asarray(
        training_history['train_losses'],     dtype=float)
    val_losses = np.asarray(training_history['val_losses'],       dtype=float)
    epochs = np.arange(1, len(train_accuracies) + 1)

    def _smooth(x, k=3):
        if len(x) < k:
            return x
        w = np.ones(k)/k
        y = np.convolve(x, w, mode='valid')
        pad = (len(x) - len(y)) // 2
        return np.pad(y, (pad, len(x)-len(y)-pad), mode='edge')

    def _roll_minmax(x, k=5):
        if k < 2 or len(x) < k:
            return x, x
        from collections import deque
        dmin, dmax, qmin, qmax, xs = [], [], deque(), deque(), x.tolist()
        for i, v in enumerate(xs):
            while qmin and xs[qmin[-1]] >= v:
                qmin.pop()
            qmin.append(i)
            while qmax and xs[qmax[-1]] <= v:
                qmax.pop()
            qmax.append(i)
            if qmin[0] <= i-k:
                qmin.popleft()
            if qmax[0] <= i-k:
                qmax.popleft()
            dmin.append(xs[qmin[0]])
            dmax.append(xs[qmax[0]])
        return np.array(dmin), np.array(dmax)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=FIGSIZE)

    # --- –¢–æ—á–Ω–æ—Å—Ç—å
    tr_s = _smooth(train_accuracies, k=3)
    va_s = _smooth(val_accuracies,   k=3)
    ax1.plot(epochs, tr_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')
    ax1.plot(epochs, va_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
    tr_lo, tr_hi = _roll_minmax(train_accuracies, k=5)
    va_lo, va_hi = _roll_minmax(val_accuracies,   k=5)
    ax1.fill_between(epochs, tr_lo, tr_hi, alpha=BAND_ALPHA)
    ax1.fill_between(epochs, va_lo, va_hi, alpha=BAND_ALPHA)
    ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    ax1.grid(True, alpha=GRID_ALPHA)
    ax1.legend(loc='lower right')

    # --- –ü–æ—Ç–µ—Ä–∏
    tl_s = _smooth(train_losses, k=3)
    vl_s = _smooth(val_losses,   k=3)
    ax2.plot(epochs, tl_s, linewidth=LINEWIDTH, label='–û–±—É—á–µ–Ω–∏–µ')
    ax2.plot(epochs, va_s, linewidth=LINEWIDTH, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
    tl_lo, tl_hi = _roll_minmax(train_losses, k=5)
    vl_lo, vl_hi = _roll_minmax(val_losses,   k=5)
    ax2.fill_between(epochs, tl_lo, tl_hi, alpha=BAND_ALPHA)
    ax2.fill_between(epochs, vl_lo, vl_hi, alpha=BAND_ALPHA)
    ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('–ü–æ—Ç–µ—Ä—è')
    ax2.grid(True, alpha=GRID_ALPHA)
    ax2.legend(loc='upper right')

    plt.tight_layout()
    plt.show()

    # --- –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    print("\nüîç –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:")
    final_train_acc = float(train_accuracies[-1])
    final_val_acc = float(val_accuracies[-1])
    gap = final_train_acc - final_val_acc
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {final_train_acc:.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_acc:.2f}%")
    print(f"–†–∞–∑—Ä—ã–≤ (Train - Val): {gap:.2f}%")
    if gap < 3:
        print("‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç")
    elif gap < 8:
        print("‚ö†Ô∏è –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    elif gap < 15:
        print("üî∂ –£–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    else:
        print("‚ùå –°–∏–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    return gap


def plot_confusion_matrix(true_labels, predictions, classes):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (–ø—Ä–æ—Ü–µ–Ω—Ç—ã + –∞–±—Å–æ–ª—é—Ç—ã, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Å—Ç–∏–Ω–µ)
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix

    FIGSIZE = (8, 6)
    GRID_ALPHA = 0.3

    cm_abs = confusion_matrix(true_labels, predictions)
    with np.errstate(invalid='ignore'):
        cm = cm_abs / cm_abs.sum(axis=1, keepdims=True)
    cm = np.nan_to_num(cm)

    plt.figure(figsize=FIGSIZE)
    ax = sns.heatmap(cm, annot=False, cmap='Blues',
                     xticklabels=classes, yticklabels=classes,
                     vmin=0.0, vmax=1.0, cbar_kws={"label": "–î–æ–ª—è –ø–æ –∏—Å—Ç–∏–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É"})
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            txt = f"{cm[i, j]*100:.1f}%\n({cm_abs[i, j]})"
            ax.text(j+0.5, i+0.5, txt, ha='center',
                    va='center', fontsize=9, color='black')

    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –ø—É—Ç–∞–Ω–∏—Ü—ã (–æ—à–∏–±–æ–∫)')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.grid(False)  # —É heatmap —Å–≤–æ—è —Å–µ—Ç–∫–∞
    plt.tight_layout()
    plt.show()

# ============================================================================
# 6. –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø - –°–†–ê–í–ù–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†
# ============================================================================

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–∞ MNIST
    """
    print("=" * 80)
    print("–ì–õ–£–ë–û–ö–û–ï –û–ë–£–ß–ï–ù–ò–ï –ù–ê MNIST: –°–†–ê–í–ù–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†")
    print("=" * 80)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST...")
    train_loader, val_loader, test_loader, classes = load_mnist_data(
        batch_size=128, val_split=0.1)
    print(f"–ö–ª–∞—Å—Å—ã: {classes}")

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    models_config = [
        {
            'name': 'SimpleCNN_MNIST',
            'model': SimpleCNN_MNIST(num_classes=10),
            'description': '–ü—Ä–æ—Å—Ç–∞—è CNN —Å 3 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏ —Å–ª–æ—è–º–∏'
        },
        {
            'name': 'AdvancedCNN',
            'model': AdvancedCNN(num_classes=10),
            'description': '–£–ª—É—á—à–µ–Ω–Ω–∞—è CNN —Å BatchNorm –∏ 4 —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏ —Å–ª–æ—è–º–∏'
        },
        # {
        #     'name': 'ResNetLike',
        #     'model': ResNetLike(num_classes=10),
        #     'description': 'ResNet-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å residual –±–ª–æ–∫–∞–º–∏'
        # }
    ]

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    all_results = {}

    # 2. –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for i, config in enumerate(models_config, 1):
        print(f"\n{'='*60}")
        print(f"2.{i} –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò: {config['name']}")
        print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {config['description']}")
        print(f"{'='*60}")

        model = config['model'].to(device)

        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in model.parameters())
        print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {total_params:,}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        criterion = nn.CrossEntropyLoss()  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä

        # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        start_time = time.time()

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        training_history = train_model(
            model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10
        )

        training_time = time.time() - start_time
        print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
        print(f"\n–û—Ü–µ–Ω–∫–∞ {config['name']} –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")
        predictions, true_labels, softmax_probs, logits = evaluate_model_with_metrics(
            model, test_loader, device, classes
        )

        # –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
        test_accuracy = accuracy_score(true_labels, predictions) * 100

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_results[config['name']] = {
            'model': model,
            'training_history': training_history,
            'test_accuracy': test_accuracy,
            'predictions': predictions,
            'true_labels': true_labels,
            'softmax_probs': softmax_probs,
            'training_time': training_time,
            'total_params': total_params
        }

        print(f"üéØ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å {config['name']}: {test_accuracy:.2f}%")

    # 3. –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
    print(f"\n{'='*80}")
    print("3. –°–†–ê–í–ù–ï–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†")
    print(f"{'='*80}")

    # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\nüìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:")
    print("-" * 80)
    print(f"{'–ú–æ–¥–µ–ª—å':<15} {'–ü–∞—Ä–∞–º–µ—Ç—Ä—ã':<12} {'–í—Ä–µ–º—è':<8} {'–í–∞–ª.':<6} {'–¢–µ—Å—Ç':<6}")
    print("-" * 80)

    best_model_name = None
    best_accuracy = 0.0

    for name, results in all_results.items():
        params = results['total_params']
        train_time = results['training_time']
        val_acc = results['training_history']['best_val_accuracy']
        test_acc = results['test_accuracy']

        print(f"{name:<15} {params:<12,} {train_time:<8.1f} {val_acc:<6.1f} {test_acc:<6.1f}")

        if test_acc > best_accuracy:
            best_accuracy = test_acc
            best_model_name = name

    print("-" * 80)
    print(f"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} (—Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%)")

    # 4. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
    print(f"\n{'='*60}")
    print(f"4. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name}")
    print(f"{'='*60}")

    best_results = all_results[best_model_name]
    predictions = best_results['predictions']
    true_labels = best_results['true_labels']
    softmax_probs = best_results['softmax_probs']

    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print(f"\nüìä –ë–ê–ó–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï:")
    print(f"Top-1 Accuracy: {best_results['test_accuracy']:.2f}%")

    # Confidence Scores
    confidence_scores = calculate_confidence_scores(softmax_probs)
    avg_confidence = np.mean(confidence_scores)
    print(f"–°—Ä–µ–¥–Ω–∏–π Confidence Score: {avg_confidence:.3f}")

    # Top-K Accuracy
    print(f"\nüéØ TOP-K ACCURACY:")
    for k in [1, 3, 5]:
        top_k_acc = calculate_top_k_accuracy(softmax_probs, true_labels, k=k)
        print(f"Top-{k} Accuracy: {top_k_acc:.2f}%")

    # –≠–Ω—Ç—Ä–æ–ø–∏—è
    entropy_values = calculate_entropy(softmax_probs)
    avg_entropy = np.mean(entropy_values)
    print(f"\nüîÄ –≠–ù–¢–†–û–ü–ò–Ø (–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å):")
    print(f"–°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {avg_entropy:.3f}")

    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞
    ece, _ = calculate_calibration_error(
        softmax_probs, predictions, true_labels)
    print(f"\n‚öñÔ∏è –ö–ê–õ–ò–ë–†–û–í–ö–ê:")
    print(f"Expected Calibration Error (ECE): {ece:.3f}")

    # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    class_report = classification_report(true_labels, predictions,
                                         target_names=classes, digits=3)
    print(class_report)

    # 5. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
    print(f"\n{'='*50}")
    print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò")
    print(f"{'='*50}")

    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    overfitting_gap = plot_training_history(best_results['training_history'])

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    plot_confusion_matrix(true_labels, predictions, classes)

    # 6. –í–´–í–û–î–´
    print(f"\n{'='*80}")
    print("–í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print(f"{'='*80}")

    print(f"\nüèÜ –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {best_model_name}")
    print(f"   - –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {best_results['total_params']:,}")
    print(f"   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {best_results['training_time']:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"\nüìà –û–°–û–ë–ï–ù–ù–û–°–¢–ò –ö–ê–ñ–î–û–ô –ê–†–•–ò–¢–ï–ö–¢–£–†–´:")

    for name, results in all_results.items():
        test_acc = results['test_accuracy']
        params = results['total_params']
        train_time = results['training_time']

        print(f"\n{name}:")
        print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: {test_acc:.2f}%")
        print(f"   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params:,}")
        print(f"   - –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time:.2f} —Å–µ–∫—É–Ω–¥")
        if name == 'SimpleCNN_MNIST':
            print("   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: –ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –±—ã—Å—Ç—Ä–∞—è –≤ –æ–±—É—á–µ–Ω–∏–∏")
            print("   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤")
        elif name == 'AdvancedCNN':
            print("   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: BatchNorm –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –±–æ–ª—å—à–µ —Å–ª–æ–µ–≤")
            print("   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Ö–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ residual connections")
        elif name == 'ResNetLike':
            print("   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: Residual –±–ª–æ–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            print("   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")

    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print(f"   - –î–ª—è production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ {best_model_name}")
    print("   - –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –µ—â–µ –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
    print("   - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (SGD —Å momentum)")
    print("   - –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

# ============================================================================
# 7. –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
# ============================================================================

if __name__ == "__main__":
    main()
