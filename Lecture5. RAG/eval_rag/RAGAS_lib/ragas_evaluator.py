"""
RAGAS Evaluator —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
"""
from typing import List, Dict, Any, Optional
import pandas as pd
import asyncio

from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import (
    LLMContextPrecisionWithReference,
    LLMContextRecall,
    ResponseRelevancy,
    Faithfulness
)
from ragas_proxy import create_proxy_llm, create_proxy_embeddings


class RagasEvaluator:
    """RAGAS Evaluator —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏"""

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        use_proxy: bool = False,
        proxy_api_key: Optional[str] = None,
        proxy_base_url: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            use_proxy: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏
            proxy_api_key: API –∫–ª—é—á –ø—Ä–æ–∫—Å–∏
            proxy_base_url: URL –ø—Ä–æ–∫—Å–∏
        """
        self.model = model
        self.temperature = temperature
        self.metric_configs = {}

        # –°–æ–∑–¥–∞–µ–º LLM –∏ embeddings
        if use_proxy:
            print(f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {proxy_base_url}")
            self.evaluator_llm = create_proxy_llm(
                model=model,
                api_key=proxy_api_key or "sk-proxy-your-key",
                base_url=proxy_base_url or "http://5.11.83.110:8000",
                temperature=temperature
            )
            self.evaluator_embeddings = create_proxy_embeddings(
                api_key=proxy_api_key or "sk-proxy-your-key",
                base_url=proxy_base_url or "http://5.11.83.110:8000"
            )
        else:
            # –û–±—ã—á–Ω—ã–π OpenAI
            from langchain_openai import ChatOpenAI, OpenAIEmbeddings
            from ragas.llms import LangchainLLMWrapper
            from ragas.embeddings import LangchainEmbeddingsWrapper

            llm = ChatOpenAI(model=model, temperature=temperature)
            self.evaluator_llm = LangchainLLMWrapper(llm)

            embeddings = OpenAIEmbeddings()
            self.evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)

        print(f"‚úÖ RAGAS –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {model}")

    def configure_metrics(self, metrics_config: Dict[str, Dict[str, Any]]):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–µ—Ç—Ä–∏–∫"""
        self.metric_configs = metrics_config

    def _create_metric(self, metric_name: str):
        """–°–æ–∑–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É"""
        if metric_name == 'context_precision':
            return LLMContextPrecisionWithReference(llm=self.evaluator_llm)
        elif metric_name == 'context_recall':
            return LLMContextRecall(llm=self.evaluator_llm)
        elif metric_name == 'response_relevancy':
            return ResponseRelevancy(
                llm=self.evaluator_llm,
                embeddings=self.evaluator_embeddings
            )
        elif metric_name == 'faithfulness':
            return Faithfulness(llm=self.evaluator_llm)

    async def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: str
    ) -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∏—Ç—å –æ–¥–∏–Ω —Å–ª—É—á–∞–π

        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            answer: –û—Ç–≤–µ—Ç RAG —Å–∏—Å—Ç–µ–º—ã
            contexts: –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            ground_truth: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –º–µ—Ç—Ä–∏–∫
        """
        sample = SingleTurnSample(
            user_input=question,
            response=answer,
            reference=ground_truth,
            retrieved_contexts=contexts
        )

        results = {}

        for metric_name, config in self.metric_configs.items():
            if not config.get('enabled', True):
                continue

            print(f"   –û—Ü–µ–Ω–∫–∞: {metric_name}")
            metric = self._create_metric(metric_name)
            score = await metric.single_turn_ascore(sample)
            results[metric_name] = float(score)

        return results

    def evaluate_batch(self, test_cases: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        Args:
            test_cases: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏:
                - question: str
                - answer: str
                - contexts: List[str]
                - ground_truth: str

        Returns:
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        async def evaluate_all():
            all_results = []
            for i, case in enumerate(test_cases, 1):
                print(f"\nüìù {i}/{len(test_cases)}: {case['question'][:50]}...")

                scores = await self.evaluate_single(
                    question=case['question'],
                    answer=case['answer'],
                    contexts=case['contexts'],
                    ground_truth=case['ground_truth']
                )

                scores['question'] = case['question']
                all_results.append(scores)

            return all_results

        results = asyncio.run(evaluate_all())
        return pd.DataFrame(results)


# ==================== –ü–†–ò–ú–ï–† ====================

if __name__ == "__main__":

    # –ü—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π!
    test_cases = [
        {
            'question': "What is the capital of France?",
            'answer': "Paris is the capital of France.",
            'contexts': ["Paris is the capital and largest city of France."],
            'ground_truth': "Paris"
        },

        # –ü—Ä–∏–º–µ—Ä 2: –•–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        {
            'question': "Who invented the telephone?",
            'answer': "Alexander Graham Bell invented the telephone in 1876.",
            'contexts': [
                "Alexander Graham Bell was a Scottish-born inventor who is credited with inventing the telephone.",
                "The first successful telephone call was made on March 10, 1876."
            ],
            'ground_truth': "Alexander Graham Bell"
        }
    ]

    # –ü—Ä–∏–º–µ—Ä 1: –° –æ–±—ã—á–Ω–æ–π OpenAI
    print("="*60)
    print("üìä –ü—Ä–∏–º–µ—Ä 1: –û–±—ã—á–Ω–∞—è OpenAI")

    evaluator = RagasEvaluator(model="gpt-4o-mini", use_proxy=False)

    evaluator.configure_metrics({
        'faithfulness': {'enabled': True, 'threshold': 0.7},
        'response_relevancy': {'enabled': True, 'threshold': 0.7}
    })

    df = evaluator.evaluate_batch(test_cases)
    print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(df)

    # –ü—Ä–∏–º–µ—Ä 2: –° –ø—Ä–æ–∫—Å–∏
    print("\n" + "="*60)
    print("üìä –ü—Ä–∏–º–µ—Ä 2: –ß–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏")

    evaluator_proxy = RagasEvaluator(
        model="gpt-4o-mini",
        use_proxy=True,
        proxy_api_key="sk-proxy-your-key",
        proxy_base_url="http://5.11.83.110:8000"
    )

    evaluator_proxy.configure_metrics({
        'faithfulness': {'enabled': True},
        'response_relevancy': {'enabled': True, 'threshold': 0.7}
    })

    df = evaluator_proxy.evaluate_batch(test_cases)
    print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(df)

    # –ü—Ä–∏–º–µ—Ä 3: –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–ª—É—á–∞—è –Ω–∞–ø—Ä—è–º—É—é
    print("\n" + "="*60)
    print("üìä –ü—Ä–∏–º–µ—Ä 3: –û–¥–∏–Ω —Å–ª—É—á–∞–π")

    async def test_single():
        scores = await evaluator.evaluate_single(
            question="What is RAG?",
            answer="RAG stands for Retrieval-Augmented Generation.",
            contexts=[
                "RAG is a technique that combines retrieval and generation."],
            ground_truth="RAG is Retrieval-Augmented Generation"
        )
        print(f"\n‚úÖ –û—Ü–µ–Ω–∫–∏: {scores}")

    asyncio.run(test_single())
