from typing import List, Union, Optional
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric
)
from deepeval import evaluate
from deepeval_custom_llm import create_proxy_model, ProxyLLM


def create_metrics(
    model: Union[str, ProxyLLM],
    metrics_list: List[str] = None,
    threshold: float = 0.7
):
    if metrics_list is None:
        metrics_list = ['answer_relevancy', 'faithfulness']

    METRICS = {
        'answer_relevancy': AnswerRelevancyMetric,
        'faithfulness': FaithfulnessMetric,
        'contextual_relevancy': ContextualRelevancyMetric,
        'contextual_recall': ContextualRecallMetric,
        'contextual_precision': ContextualPrecisionMetric
    }

    metrics = []
    for metric_name in metrics_list:
        if metric_name in METRICS:
            metric = METRICS[metric_name](
                threshold=threshold,
                model=model,
                include_reason=True
            )
            metrics.append(metric)

    return metrics


if __name__ == "__main__":

    # ============= –ü–†–ò–ú–ï–† 1: –° –æ–±—ã—á–Ω–æ–π OpenAI =============
    print("üìä –ü—Ä–∏–º–µ—Ä 1: –û–±—ã—á–Ω–∞—è OpenAI –º–æ–¥–µ–ª—å")

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –æ–±—ã—á–Ω–æ–π –º–æ–¥–µ–ª—å—é
    metrics = create_metrics(
        model="gpt-4o-mini",  # ‚Üê –û–±—ã—á–Ω–∞—è OpenAI
        metrics_list=['answer_relevancy', 'faithfulness'],
        threshold=0.7
    )

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç –∫–µ–π—Å
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        actual_output="We offer a 30-day full refund at no extra cost.",
        expected_output="You are eligible for a 30 day full refund at no extra cost.",
        retrieval_context=[
            "All customers are eligible for a 30 day full refund at no extra cost."]
    )

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º (DeepEval —Å–∞–º –≤—ã–≤–µ–¥–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
    evaluate(test_cases=[test_case], metrics=metrics)

    # ============= –ü–†–ò–ú–ï–† 2: –° –ü–†–û–ö–°–ò =============
    print("\n" + "="*60)
    print("üìä –ü—Ä–∏–º–µ—Ä 2: –ö–∞—Å—Ç–æ–º–Ω–∞—è –ø—Ä–æ–∫—Å–∏ –º–æ–¥–µ–ª—å")

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–∫—Å–∏ –º–æ–¥–µ–ª—å
    proxy_model = create_proxy_model(
        model="gpt-4o-mini",
        api_key="sk-proxy-your-key",
        base_url="http://5.11.83.110:8000"
    )

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –ø—Ä–æ–∫—Å–∏ –º–æ–¥–µ–ª—å—é
    metrics = create_metrics(
        model=proxy_model,  # ‚Üê –ö–∞—Å—Ç–æ–º–Ω–∞—è –ø—Ä–æ–∫—Å–∏ –º–æ–¥–µ–ª—å
        metrics_list=['answer_relevancy', 'faithfulness'],
        threshold=0.7
    )

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º
    evaluate(test_cases=[test_case], metrics=metrics)

    # ============= –ü–†–ò–ú–ï–† 3: –ë–µ–∑ wrapper —Ñ—É–Ω–∫—Ü–∏–∏ =============
    print("\n" + "="*60)
    print("üìä –ü—Ä–∏–º–µ—Ä 3: –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DeepEval (–≤–æ–æ–±—â–µ –±–µ–∑ –æ–±–µ—Ä—Ç–∫–∏)")

    # –ü—Ä—è–º–æ —Å–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
    metric = AnswerRelevancyMetric(
        model=proxy_model,  # –ú–æ–∂–Ω–æ proxy_model –∏–ª–∏ "gpt-4o-mini"
        threshold=0.7,
        include_reason=True
    )

    # –ò–∑–º–µ—Ä—è–µ–º
    metric.measure(test_case)

    # –°–º–æ—Ç—Ä–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nScore: {metric.score:.3f}")
    print(f"Reason: {metric.reason}")
