from fastapi import APIRouter, HTTPException, Depends, Query
from typing import List, Optional, Dict, Any
from datetime import datetime
from app.config import settings 

from app.models.chat import (
    ChatRequest, ChatResponse,
    RelevantChunk
)
from app.services.embedding_service import get_embedding_service
from app.services.vector_store import get_vector_store_service
from app.services.llm_service import get_llm_service
from app.services.document_service import DocumentService, get_document_service as get_global_service
import logging
from app.utils.auth import verify_api_key

logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º —Ä–æ—É—Ç–µ—Ä
router = APIRouter(prefix="/chat", tags=["chat"])

def get_document_service() -> DocumentService:
    return get_global_service()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä


@router.post("/", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    embedding_service=Depends(get_embedding_service),
    vector_store=Depends(get_vector_store_service),
    llm_service=Depends(get_llm_service),
    document_service: DocumentService = Depends(get_document_service),
    api_key: str = Depends(verify_api_key) 
):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ RAG —á–∞—Ç

    - **message**: —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
    - **document_ids**: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - **max_relevant_chunks**: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    - **similarity_threshold**: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
    - **temperature**: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM
    - **llm_provider**: –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM (openai, anthropic)
    - **model_name**: –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å (gpt-4, claude-3-sonnet-20240229, etc.)
    """

    if not api_key:
        raise HTTPException(status_code=401, detail="Unauthorized")
    try:
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
        relevant_chunks = await find_relevant_chunks(
            request.message,
            request,
            embedding_service,
            vector_store,
            document_service
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
        rag_response = await llm_service.generate_rag_response(
            query=request.message,
            relevant_chunks=relevant_chunks,
            temperature=request.temperature or 0.7,
            max_tokens=None,
            llm_provider=request.llm_provider.value if request.llm_provider else None,
            model_name=request.model_name
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–ë–ï–ó –æ–±—Ä–µ–∑–∞–Ω–∏—è)
        sources = []
        for chunk in relevant_chunks:
            metadata = chunk.get('metadata', {})
            sources.append({
                "chunk_id": chunk.get('chunk_id'),
                "document_id": metadata.get('document_id'),
                "document_title": metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'),
                "similarity": chunk.get('similarity', 0),
                "content": chunk['text']  # –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è
            })

        return ChatResponse(
            message_id="msg_" + str(int(datetime.utcnow().timestamp())),
            session_id="single_session",  # –ó–∞–≥–ª—É—à–∫–∞
            content=rag_response["answer"],
            sources=sources,
            metadata={
                "sources_count": len(sources),
                "context_length": rag_response["context_length"],
                "llm_provider": rag_response["llm_provider"],
                "model_used": rag_response["model_used"]
            }
        )

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

async def find_relevant_chunks(
    query: str,
    request: ChatRequest,
    embedding_service,
    vector_store,
    document_service: DocumentService,
    api_key: str = Depends(verify_api_key)
) -> List[Dict[str, Any]]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""

    if not api_key:
        raise HTTPException(status_code=401, detail="Unauthorized")
    try:
        threshold = request.similarity_threshold if request.similarity_threshold is not None else settings.SIMILARITY_THRESHOLD
        max_chunks = request.max_relevant_chunks if request.max_relevant_chunks is not None else settings.MAX_RELEVANT_CHUNKS
        
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}' (threshold={threshold}, max={max_chunks})")
        
        # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = await embedding_service.create_embedding_for_query(query)
        logger.info(f"‚úÖ Embedding —Å–æ–∑–¥–∞–Ω: {len(query_embedding)} –∏–∑–º–µ—Ä–µ–Ω–∏–π")

        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
        relevant_chunks = await vector_store.search_similar_chunks(
            query_embedding=query_embedding,
            top_k=max_chunks,
            similarity_threshold=threshold
        )
        
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(relevant_chunks)}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if request.document_ids:
            relevant_chunks = [
                chunk for chunk in relevant_chunks
                if chunk.get('metadata', {}).get('document_id') in request.document_ids
            ]
            logger.info(f"üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(relevant_chunks)}")

        # –û–±–æ–≥–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        for chunk in relevant_chunks:
            doc_id = chunk.get('metadata', {}).get('document_id')
            document = document_service.get_document(doc_id)
            if document:
                chunk['metadata']['source'] = document.title
                chunk['metadata']['document_type'] = document.document_type.value
            
            logger.info(f"   Chunk: similarity={chunk.get('similarity', 0):.4f}, doc={chunk['metadata'].get('source', 'Unknown')}")

        return relevant_chunks

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}", exc_info=True)
        return []
