"""
End-to-End RAG Evaluation Pipeline with eval_lib
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Excel, –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –æ—Ç RAG –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É
"""

import sys
sys.path.insert(
    0, '/Users/aleksandrmeskov/Desktop/AI evaluation/AI_practice/Lecture5/eval_rag')

sys.path.insert(0, '/Users/aleksandrmeskov/Desktop/Projects/Eval-ai-library')

import asyncio  # noqa: E402
import time  # noqa: E402
import pandas as pd  # noqa: E402
from typing import List  # noqa: E402
from eval_lib import (  # noqa: E402
    evaluate,
    EvalTestCase,
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualRelevancyMetric,
    CustomLLMClient
)
from dataset_parser import DatasetParser  # noqa: E402
from rag_connector import RAGConnector  # noqa: E402
from custom_proxy_llm import create_proxy_llm  # noqa: E402


async def evaluate_rag_from_excel(
    excel_path: str,
    rag_connector: RAGConnector,
    metrics_list: List[str],
    model: str | CustomLLMClient = "gpt-4o-mini",
    threshold: float = 0.7,
    temperature: float = 0.5,
    sleep_time: float = 0.1,
    verbose: bool = True,
    show_dashboard: bool = False,
    session_name: str = "Evaluation Session"
):

    # ============= –®–ê–ì 1: –ü–∞—Ä—Å–∏–º Excel =============
    print(f"\n –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª–∞...")

    parser = DatasetParser()
    df = parser.load_dataset(excel_path)

    if df is None:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    info = parser.validate_dataset(df)
    print(f"\n –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {info['total_rows']}")
    print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä: {info['valid_pairs']}")

    # –ü—Ä–µ–≤—å—é
    parser.preview_dataset(df, n=2)

    # ============= –®–ê–ì 2: –ü–æ–ª—É—á–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏ expected –æ—Ç–≤–µ—Ç—ã =============
    print(f"\n –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    questions = parser.get_questions(df)
    expected_responses = parser.get_expected_responses(df)

    print(f" –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # ============= –®–ê–ì 3: –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ RAG =============
    print(f"\n –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã...")

    test_cases = []

    for i, question in enumerate(questions, 1):
        print(f"\n  [{i}/{len(questions)}] {question[:60]}...")

        # –ó–∞–ø—Ä–æ—Å –∫ RAG
        rag_response = rag_connector.query(question)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏
        if 'error' in rag_response:
            print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ RAG: {rag_response['error']}")
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        actual_output = rag_response.get('content', '')
        sources = rag_response.get('sources', [])
        retrieval_context = [s.get('content', '')
                             for s in sources if s.get('content')]

        print(f"      ‚úÖ –û—Ç–≤–µ—Ç: {actual_output[:60]}...")
        print(f"      üìö –ö–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {len(retrieval_context)}")

        # –ü–æ–ª—É—á–∞–µ–º expected_output
        expected = expected_responses[i-1] if i - \
            1 < len(expected_responses) else ""

        # –°–æ–∑–¥–∞–µ–º EvalTestCase
        test_case = EvalTestCase(
            input=question,
            actual_output=actual_output,
            expected_output=expected,
            retrieval_context=retrieval_context
        )

        test_cases.append(test_case)

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(sleep_time)

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç –∫–µ–π—Å–æ–≤")

    # ============= –®–ê–ì 4: –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ =============
    print(f"\n –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏...")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {metrics_list}")
    print(f"   –ü–æ—Ä–æ–≥: {threshold}")
    print(f"   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
    model_name = model.get_model_name() if isinstance(
        model, CustomLLMClient) else model
    print(f"   –ú–æ–¥–µ–ª—å: {model_name}")

    metrics = []
    metric_classes = {
        'answer_relevancy': AnswerRelevancyMetric,
        'faithfulness': FaithfulnessMetric,
        'contextual_relevancy': ContextualRelevancyMetric
    }

    for metric_name in metrics_list:
        if metric_name in metric_classes:
            metric = metric_classes[metric_name](
                model=model,
                threshold=threshold,
                temperature=temperature,
                verbose=verbose
            )
            metrics.append(metric)
            print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞: {metric_name}")

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(metrics)} –º–µ—Ç—Ä–∏–∫")

    # ============= –®–ê–ì 5: –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É =============
    print(f"\nüß™ –®–∞–≥ 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏...")
    print(f"   –¢–µ—Å—Ç –∫–µ–π—Å–æ–≤: {len(test_cases)}")
    print(f"   –ú–µ—Ç—Ä–∏–∫: {len(metrics)}")

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
        results = await evaluate(
            test_cases=test_cases,
            metrics=metrics,
            verbose=verbose,
            show_dashboard=show_dashboard,
            session_name=session_name
        )

        return results

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return None


# ==================== –¢–ï–°–¢–û–í–´–ï –°–¶–ï–ù–ê–†–ò–ò ====================

async def scenario_1_standard_openai():
    """–°—Ü–µ–Ω–∞—Ä–∏–π 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è OpenAI –º–æ–¥–µ–ª—å"""

    print("\n" + "="*70)
    print("üìã –°–¶–ï–ù–ê–†–ò–ô 1: –û—Ü–µ–Ω–∫–∞ —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π OpenAI –º–æ–¥–µ–ª—å—é")
    print("="*70)

    rag_connector = RAGConnector(
        endpoint_url="http://5.11.83.110:8002/api/v1/chat/",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        timeout=30
    )

    excel_path = "data/evaluation_dataset.xlsx"

    metrics_to_use = [
        'answer_relevancy',
        'faithfulness',
        'contextual_relevancy'
    ]

    results = await evaluate_rag_from_excel(
        excel_path=excel_path,
        rag_connector=rag_connector,
        metrics_list=metrics_to_use,
        model="gpt-4o-mini",  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è OpenAI
        threshold=0.7,
        temperature=0.5,
        sleep_time=0.1,
        verbose=True,
        show_dashboard=True,
        session_name="OpenAI Model Evaluation"

    )

    return results


async def scenario_2_custom_model():
    """–°—Ü–µ–Ω–∞—Ä–∏–π 2: –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å (–ø—Ä–æ–∫—Å–∏)"""

    # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å
    custom_model = create_proxy_llm(
        model="gpt-4o-mini",
        api_key="sk-proxy-your-key",
        base_url="http://5.11.83.110:8000"
    )

    rag_connector = RAGConnector(
        endpoint_url="http://5.11.83.110:8002/api/v1/chat/",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        timeout=30
    )

    excel_path = "data/evaluation_dataset.xlsx"

    metrics_to_use = [
        'answer_relevancy',
        'faithfulness',
        'contextual_relevancy'
    ]

    results = await evaluate_rag_from_excel(
        excel_path=excel_path,
        rag_connector=rag_connector,
        metrics_list=metrics_to_use,
        model=custom_model,  # –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å
        threshold=0.7,
        temperature=0.5,
        sleep_time=0.1,
        verbose=True,
        show_dashboard=True,
        session_name="OpenAI Model Evaluation"
    )

    return results


async def scenario_3_strict_evaluation():
    """–°—Ü–µ–Ω–∞—Ä–∏–π 4: –°—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ (–Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)"""

    custom_model = create_proxy_llm(
        model="gpt-4o-mini",
        api_key="sk-proxy-your-key",
        base_url="http://5.11.83.110:8000"
    )

    rag_connector = RAGConnector(
        endpoint_url="http://5.11.83.110:8002/api/v1/chat/",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        timeout=30
    )

    excel_path = "data/evaluation_dataset.xlsx"

    metrics_to_use = [
        'answer_relevancy',
        'faithfulness'
    ]

    results = await evaluate_rag_from_excel(
        excel_path=excel_path,
        rag_connector=rag_connector,
        metrics_list=metrics_to_use,
        model=custom_model,
        threshold=0.8,  # –í—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥
        temperature=0.1,  # STRICT: –≤—Å–µ –≤–µ—Ä–¥–∏–∫—Ç—ã –≤–∞–∂–Ω—ã
        sleep_time=0.1,
        verbose=True,
        show_dashboard=True,
        session_name="Strict Evaluation"
    )

    return results


async def scenario_4_lenient_evaluation():
    """–°—Ü–µ–Ω–∞—Ä–∏–π 5: –ú—è–≥–∫–∞—è –æ—Ü–µ–Ω–∫–∞ (–≤—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)"""

    custom_model = create_proxy_llm(
        model="gpt-4o-mini",
        api_key="sk-proxy-your-key",
        base_url="http://5.11.83.110:8000"
    )

    rag_connector = RAGConnector(
        endpoint_url="http://5.11.83.110:8002/api/v1/chat/",
        api_key="80456142-5441-4469-b97f-1d72b7802a93",
        timeout=30
    )

    excel_path = "data/evaluation_dataset.xlsx"

    metrics_to_use = [
        'answer_relevancy',
        'faithfulness'
    ]

    results = await evaluate_rag_from_excel(
        excel_path=excel_path,
        rag_connector=rag_connector,
        metrics_list=metrics_to_use,
        model=custom_model,
        threshold=0.6,  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
        temperature=1.0,  # LENIENT: —Ñ–æ–∫—É—Å –Ω–∞ —Ö–æ—Ä–æ—à–∏—Ö –≤–µ—Ä–¥–∏–∫—Ç–∞—Ö
        sleep_time=0.1,
        verbose=True,
        show_dashboard=True,
        session_name="Lenient Evaluation"
    )

    return results


if __name__ == "__main__":

    asyncio.run(scenario_3_strict_evaluation())
