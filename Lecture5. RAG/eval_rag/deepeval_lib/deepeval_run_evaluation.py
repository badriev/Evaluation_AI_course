"""
–û—Ü–µ–Ω–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫ DeepEval
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Excel, –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –æ—Ç RAG –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É
"""

import sys
sys.path.insert(
    0, '/Users/aleksandrmeskov/Desktop/AI evaluation/AI_practice/Lecture5/eval_rag')

import time  # noqa: E402
import pandas as pd  # noqa: E402
from typing import List, Union  # noqa: E402
from deepeval.test_case import LLMTestCase  # noqa: E402
from deepeval import evaluate  # noqa: E402
from dataset_parser import DatasetParser  # noqa: E402
from rag_connector import RAGConnector  # noqa: E402
from deepeval_custom_llm import create_proxy_model, ProxyLLM  # noqa: E402
from deepeval_evaluator import create_metrics  # noqa: E402


def evaluate_rag_from_excel(
    excel_path: str,
    rag_connector: RAGConnector,
    metrics_list: List[str],
    model: Union[str, ProxyLLM] = "gpt-4o-mini",
    threshold: float = 0.7,
    sleep_time: float = 0.1
):

    # ============= –®–ê–ì 1: –ü–∞—Ä—Å–∏–º Excel =============
    print(f" –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª–∞...")

    parser = DatasetParser()
    df = parser.load_dataset(excel_path)

    if df is None:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        return

    # –ü—Ä–µ–≤—å—é
    parser.preview_dataset(df, n=2)

    # ============= –®–ê–ì 2: –ü–æ–ª—É—á–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏ expected –æ—Ç–≤–µ—Ç—ã =============
    print(f"–®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    questions = parser.get_questions(df)
    expected_responses = parser.get_expected_responses(df)

    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # ============= –®–ê–ì 3: –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ RAG =============
    print(f"–®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã...")

    test_cases = []

    for i, question in enumerate(questions, 1):
        print(f"\n  [{i}/{len(questions)}] {question[:60]}...")

        # –ó–∞–ø—Ä–æ—Å –∫ RAG
        rag_response = rag_connector.query(question)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏
        if 'error' in rag_response:
            print(f"–û—à–∏–±–∫–∞ RAG: {rag_response['error']}")
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        actual_output = rag_response.get('content', '')
        sources = rag_response.get('sources', [])
        retrieval_context = [s.get('content', '')
                             for s in sources if s.get('content')]

        print(f"–û—Ç–≤–µ—Ç: {actual_output[:60]}...")
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {len(retrieval_context)}")

        # –ü–æ–ª—É—á–∞–µ–º expected_output
        expected = expected_responses[i-1] if i - \
            1 < len(expected_responses) else ""

        # –°–æ–∑–¥–∞–µ–º LLMTestCase –¥–ª—è DeepEval
        test_case = LLMTestCase(
            input=question,
            actual_output=actual_output,
            expected_output=expected,
            retrieval_context=retrieval_context
        )

        test_cases.append(test_case)

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(sleep_time)

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç –∫–µ–π—Å–æ–≤")

    # ============= –®–ê–ì 4: –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ =============
    print(f"\nüìä –®–∞–≥ 4: –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏...")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {metrics_list}")
    print(f"   –ü–æ—Ä–æ–≥: {threshold}")

    metrics = create_metrics(
        model=model,
        metrics_list=metrics_list,
        threshold=threshold
    )

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(metrics)} –º–µ—Ç—Ä–∏–∫")

    # ============= –®–ê–ì 5: –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É =============
    print(f"\nüß™ –®–∞–≥ 5: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏...")

    try:
        # DeepEval –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–≤–µ–¥–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        evaluate(
            test_cases=test_cases,
            metrics=metrics
        )

        print(f"\n‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üåê –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ https://app.confident-ai.com")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":

    # ============= –ù–ê–°–¢–†–û–ô–ö–ê =============

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä
    rag_connector = RAGConnector(
        endpoint_url="http://5.11.83.110:8002/api/v1/chat/",
        api_key="rag-api-key",
        timeout=30
    )

    # 2. –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–∫—Å–∏ –º–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ "gpt-4o-mini")
    proxy_model = create_proxy_model(
        model="gpt-4o-mini",
        api_key="sk-proxy-your-key",
        base_url="http://5.11.83.110:8000"
    )

    # 3. –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
    excel_path = "data/evaluation_dataset.xlsx"

    # 4. –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    metrics_to_use = [
        'answer_relevancy',
        'faithfulness',
        'contextual_relevancy'
    ]

    # ============= –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò =============

    evaluate_rag_from_excel(
        excel_path=excel_path,
        rag_connector=rag_connector,
        metrics_list=metrics_to_use,
        model=proxy_model,  # –ò–ª–∏ "gpt-4o-mini" –¥–ª—è –æ–±—ã—á–Ω–æ–π OpenAI
        threshold=0.7,
        sleep_time=0.1
    )
